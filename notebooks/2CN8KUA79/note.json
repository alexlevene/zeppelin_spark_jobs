{
  "paragraphs": [
    {
      "text": "// Import our dependencies\nimport org.apache.spark.sql.SQLContext    \nimport org.apache.spark.sql.SQLContext._\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.DataFrame\n\nimport org.elasticsearch.spark.sql._\nimport org.elasticsearch.spark._\nimport org.elasticsearch.spark.rdd.Metadata._ \n\n// dependencies required for elasticsearch direct access via REST client interface\nimport org.apache.http.client.methods.HttpPost\nimport org.apache.http.entity.StringEntity\nimport org.apache.http.impl.client.DefaultHttpClient\nimport org.apache.http.impl.client.BasicResponseHandler\nimport java.util.Date\nimport java.util.Calendar\nimport org.apache.commons.lang3.time.DateUtils\nimport java.text.SimpleDateFormat\nimport scala.collection.mutable.WrappedArray\n",
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:23:52 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nimport org.apache.spark.sql.SQLContext\n\nimport org.apache.spark.sql.SQLContext._\n\nimport org.apache.spark.sql.SparkSession\n\nimport org.apache.spark.sql.DataFrame\n\nimport org.elasticsearch.spark.sql._\n\nimport org.elasticsearch.spark._\n\nimport org.elasticsearch.spark.rdd.Metadata._\n\nimport org.apache.http.client.methods.HttpPost\n\nimport org.apache.http.entity.StringEntity\n\nimport org.apache.http.impl.client.DefaultHttpClient\n\nimport org.apache.http.impl.client.BasicResponseHandler\n\nimport java.util.Date\n\nimport java.util.Calendar\n\nimport org.apache.commons.lang3.time.DateUtils\n\nimport java.text.SimpleDateFormat\n\nimport scala.collection.mutable.WrappedArray\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498079018700_853200945",
      "id": "20170505-005539_540198305",
      "dateCreated": "Jun 21, 2017 9:03:38 PM",
      "dateStarted": "Jun 21, 2017 9:23:52 PM",
      "dateFinished": "Jun 21, 2017 9:23:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val esIndexName \u003d \"exp_rjj_1_2\"\nval esHost \u003d \"elasticsearch.exp-dev.io\"\nval esPort \u003d 9200\nval resultSize \u003d 10000\nval clientCode \u003d \"DEMO\"",
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:24:32 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nesIndexName: String \u003d exp_rjj_1_2\n\nesHost: String \u003d elasticsearch.exp-dev.io\n\nesPort: Int \u003d 9200\n\nresultSize: Int \u003d 10000\n\nclientCode: String \u003d DEMO\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498079018701_852816196",
      "id": "20170505-015311_2021186754",
      "dateCreated": "Jun 21, 2017 9:03:38 PM",
      "dateStarted": "Jun 21, 2017 9:24:32 PM",
      "dateFinished": "Jun 21, 2017 9:24:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def getPersonsWithoutEncounterLifecycle(clientCode:String,esIndexName:String,esHost:String,esPort:Int,resultLimit:Int \u003d 10000) :(String,Int) \u003d {\n\n    val personsWithoutEncounterLifecycle \u003d s\"\"\"\n        {\n          \"size\": ${resultLimit},\n          \"_source\": [\"_id\"],\n          \"query\": {\n            \"constant_score\": {\n              \"filter\": {\n                \"bool\": {\n                  \"must\": [\n                    {\"term\":{\"client_code\" : \"${clientCode}\" }},\n                    {\n                      \"has_child\": {\n                        \"child_type\": \"encounter\",\n                        \"query\": {\n                          \"bool\": {\n                            \"must\": [\n                              { \"term\": {\"client_code\": \"${clientCode}\" }}\n                            ],\n                            \"must_not\":[\n                              { \"exists\": { \"field\": \"patient_lifecycle.state_flags.re_engaged\"} }\n                            ]\n                          }\n                        }\n                      } \n                    }\n                  ]\n                }\n              }\n            }\n          }\n        }\n    \"\"\"\n\n    // generate the elasticsearch request string\n    val endPoint \u003d \"/\"+ esIndexName + \"/person/_search\"\n    val request_url \u003d \"http://\" + esHost + \":\" + esPort + endPoint\n    \n    // build the apache HTTP post request\n    val post \u003d new HttpPost(request_url)\n    // set header for json request string\n    val setHeaderReturn \u003d post.setHeader(\"Content-type\",\"application/json\")\n    // ad the json request string to the http post\n    val setEntityReturn \u003d post.setEntity(new StringEntity(personsWithoutEncounterLifecycle))\n    // send the request to elasticsearch\n    val response \u003d (new DefaultHttpClient).execute(post)\n    // get the status -- this code doesn\u0027t check for HTTP/1.1 200 OK response but the final code should!\n    val status \u003d response.getStatusLine\n    // get the json response body\n    val responseBody \u003d (new BasicResponseHandler).handleResponse(response).trim.toString\n\n    // check the results to ensure there were any households returned\n    val dfCount \u003d sqlContext.read.json(sc.parallelize(responseBody::Nil)).select($\"hits.total\")\n    \n    val personTotalHits \u003d dfCount.first().getLong(0).asInstanceOf[Int]\n    \n    if (personTotalHits \u003d\u003d 0) {\n        return (null,personTotalHits)\n    }\n    \n    val df \u003d sqlContext.read.json(sc.parallelize(responseBody::Nil)).select($\"hits.hits._id\")\n    \n    val idList \u003d df.first().getAs[WrappedArray[String]](0)\n    \n    var idlist_length:Int \u003d idList.length\n    println(s\"idlist length ${idlist_length}\")\n    \n    if (idlist_length \u003d\u003d 0) {\n        return (null,idlist_length)\n    }\n    \n    val idString \u003d idList.mkString(\"\\\"\",\"\\\",\\n\\\"\",\"\\\"\")\n    \n    println (s\"returned ${resultLimit} results\")\n\n    return (idString, idlist_length)\n}",
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:25:31 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\ngetPersonsWithoutEncounterLifecycle: (clientCode: String, esIndexName: String, esHost: String, esPort: Int, resultLimit: Int)(String, Int)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498079018701_852816196",
      "id": "20170620-193524_922175258",
      "dateCreated": "Jun 21, 2017 9:03:38 PM",
      "dateStarted": "Jun 21, 2017 9:25:31 PM",
      "dateFinished": "Jun 21, 2017 9:25:31 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\ndef getEncounterSourceData(spark:SparkSession,clientCode:String,esIndexName:String,esServer:String,patientList:String):org.apache.spark.sql.DataFrame \u003d {\n\n    val sc \u003d spark.sparkContext\n    val sqlContext \u003d spark.sqlContext\n    import sqlContext.implicits._    \n    // this returns everyone who does not have patient lifecyle\n    val encounterSourceQuery \u003d s\"\"\"\n    {\n      \"query\": {\n        \"constant_score\": {\n          \"filter\": {\n            \"bool\": {\n              \"must\": [\n                {\"term\":{\"client_code\" : \"${clientCode}\" }},\n                {\n                  \"has_parent\": {\n                    \"parent_type\": \"person\",\n                    \"query\": {\n                      \"bool\": {\n                        \"must\": [\n                          { \"term\": {\"client_code\": \"${clientCode}\" }},\n                          {\"terms\": {\"_id\": [\n    ${patientList}\n                            ]\n                          }}\n                        ]\n                      }\n                    }\n                  } \n                }\n              ]\n            }\n          }\n        }\n      }\n    }\n        \"\"\"\n\n    // limit the fields that are included\n    val encounterSourceQueryOptions \u003d Map(\n    //  \"es.read.field.include\" -\u003e \"recordId,parent,admit_date,service_category,facility.code\"\n      \"es.read.field.exclude\" -\u003e \"person_key,encounter_key,admit_age,admit_source,admit_type,client_code,client_name,discharge_date,encounter_type,financial_class,msdrg,service_sub_category,diagnosis,cpt,procedure,campaigns,total_charges,total_amount_received,expected_reimbursement,direct_costs,actual_contribution_margin,expected_contribution_margin,recency_frequency,patient_lifecycle\",\n    \"es.nodes\" -\u003e esServer, \"es.write.operation\" -\u003e \"upsert\", \"es.nodes.wan.only\" -\u003e \"true\",\"es.index.auto.create\" -\u003e \"false\"\n      )\n    \n    val encountersES \u003d sqlContext.esDF(s\"${esIndexName}/encounter\", encounterSourceQuery, encounterSourceQueryOptions)\n    encountersES.cache\n    encountersES.createOrReplaceTempView(\"encountersES\")\n    return encountersES\n}\n",
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:23:52 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\ngetEncounterSourceData: (spark: org.apache.spark.sql.SparkSession, clientCode: String, esIndexName: String, esServer: String, patientList: String)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498079018702_853970443",
      "id": "20170621-205314_1023372821",
      "dateCreated": "Jun 21, 2017 9:03:38 PM",
      "dateStarted": "Jun 21, 2017 9:23:58 PM",
      "dateFinished": "Jun 21, 2017 9:23:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def buildEncounterBaseFromSource(spark:SparkSession) :org.apache.spark.sql.DataFrame \u003d {\n\n    val sc \u003d spark.sparkContext\n    val sqlContext \u003d spark.sqlContext\n    import sqlContext.implicits._    \n\n    val encounterSelect \u003d spark.sql(\"\"\"\n      SELECT\n        recordId as encounterId,\n        parent as personId,\n        admit_date as admitDate,\n        facility.code as facility,\n        service_category as serviceCategory,\n        row_number() over (partition by parent order by admit_date asc, recordId asc) as enc_seq ,\n        row_number() over (partition by parent order by admit_date asc, recordId asc) -1 as enc_prev,\n        row_number() over (partition by parent,service_category order by admit_date asc, recordId asc) as enc_seq_sc,\n        row_number() over (partition by parent,service_category order by admit_date asc, recordId asc) -1 as enc_prev_sc,\n        row_number() over (partition by parent,facility.code order by admit_date asc, recordId asc) as enc_seq_f,\n        row_number() over (partition by parent,facility.code order by admit_date asc, recordId asc) -1 as enc_prev_f\n        FROM encountersES\n        order by parent, admit_date\n    \"\"\")\n    encounterSelect.createOrReplaceTempView(\"encounter\")\n    encounterSelect.cache()\n\n    return encounterSelect\n}\n",
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:23:53 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbuildEncounterBaseFromSource: (spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498079018703_853585694",
      "id": "20170621-210202_2131397006",
      "dateCreated": "Jun 21, 2017 9:03:38 PM",
      "dateStarted": "Jun 21, 2017 9:23:59 PM",
      "dateFinished": "Jun 21, 2017 9:23:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def buildEncounterYearsBetweenFromBase(spark:SparkSession) :org.apache.spark.sql.DataFrame \u003d {\n\n    val sc \u003d spark.sparkContext\n    val sqlContext \u003d spark.sqlContext\n    import sqlContext.implicits._    \n    \n    val encounterYearsBetween \u003d spark.sql(\"\"\"\n        select cur.encounterId,\n        cur.personId,\n        cur.enc_seq,\n        cur.enc_prev,\n        case when prv.personid is null then 99 else datediff(cur.admitDate,prv.admitDate)/365.25 end as yearsBetween,\n        case when prv_f.personid is null then 99 else datediff(cur.admitDate,prv_f.admitDate)/365.25 end as yearsBetween_f,\n        case when prv_sc.personid is null then 99 else datediff(cur.admitDate,prv_sc.admitDate)/365.25 end as yearsBetween_sc,\n        prv_not_sc.yearsbetween as yearsBetween_not_sc,\n        prv_not_f.yearsbetween as yearsBetween_not_f,\n        (case when \n        (case when prv_sc.personid is null then 98 else datediff(cur.admitDate,prv_sc.admitDate)/365.25 end) \u003c\n        (case when prv_f.personid is null then 99 else datediff(cur.admitDate,prv_f.admitDate)/365.25 end)\n        then (case when prv_sc.personid is null then 99 else datediff(cur.admitDate,prv_sc.admitDate)/365.25 end)\n        else (case when prv_f.personid is null then 99 else datediff(cur.admitDate,prv_f.admitDate)/365.25 end) end) as yearsBetween_f_or_sc\n        from encounter cur \n        left join encounter prv on cur.personid \u003d prv.personid and cur.enc_prev \u003d prv.enc_seq\n        left join encounter prv_f on cur.personid \u003d prv_f.personid and cur.enc_prev_f \u003d prv_f.enc_seq_f and cur.facility \u003d prv_f.facility\n        left join encounter prv_sc on cur.personid \u003d prv_sc.personid and cur.enc_prev_sc \u003d prv_sc.enc_seq_sc and cur.serviceCategory \u003d prv_sc.serviceCategory\n        left join (\n        select encounterid,case when personid is null then 98 else yearsbetween end as yearsbetween from (\n        select cur.encounterid,prv.personid,\n        datediff(cur.admitdate,prv.admitdate)/365.25 as yearsbetween,\n        row_number() over (partition by cur.personid,cur.serviceCategory,cur.enc_seq order by prv.enc_seq desc) as not_sc_seq\n        from encounter cur left join encounter prv \n        on prv.personid \u003d cur.personid\n        and cur.serviceCategory \u003c\u003e prv.serviceCategory\n        and cur.enc_seq \u003e prv.enc_seq\n        ) where not_sc_seq \u003d 1\n        ) prv_not_sc on cur.encounterid \u003d prv_not_sc.encounterid\n        left join (\n        select encounterid,case when personid is null then 99 else yearsbetween end as yearsbetween from (\n        select cur.encounterid,prv.personid,\n        datediff(cur.admitdate,prv.admitdate)/365.25 as yearsbetween,\n        row_number() over (partition by cur.personid,cur.facility,cur.enc_seq order by prv.enc_seq desc) as not_f_seq\n        from encounter cur left join encounter prv \n        on prv.personid \u003d cur.personid\n        and cur.facility \u003c\u003e prv.facility\n        and cur.enc_seq \u003e prv.enc_seq\n        ) where not_f_seq \u003d 1\n        ) prv_not_f on cur.encounterid \u003d prv_not_f.encounterid\n    \"\"\")\n    \n    encounterYearsBetween.cache\n    encounterYearsBetween.createOrReplaceTempView(\"encounterYearsBetween\")\n\n    return encounterYearsBetween\n}\n",
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:23:53 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbuildEncounterYearsBetweenFromBase: (spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498079055072_-78447997",
      "id": "20170621-210415_1666968527",
      "dateCreated": "Jun 21, 2017 9:04:15 PM",
      "dateStarted": "Jun 21, 2017 9:23:59 PM",
      "dateFinished": "Jun 21, 2017 9:24:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def buildEncounterLifecycleFlagsFromYearsBetween(spark:SparkSession) :org.apache.spark.sql.DataFrame \u003d {\n\n    val sc \u003d spark.sparkContext\n    val sqlContext \u003d spark.sqlContext\n    import sqlContext.implicits._    \n\n    \n    val encounterLifecycleFlags \u003d spark.sql(\"\"\"\n    select encounterid,\n    personId,\n    enc_seq,\n    enc_prev,\n    yearsBetween,\n    yearsBetween_f,\n    yearsBetween_sc,\n    yearsBetween_not_sc,\n    yearsBetween_not_f,\n    yearsBetween_f_or_sc,\n    -- new patient if they did not have any medical encounter in the previous 3 years\n    case when yearsbetween \u003e 3 then 1 else 0 end as new,\n    -- re-engaged patient if they did have an encounter in the previous 3 years and did not have an encounter in the last year\n    case when yearsbetween \u003e 1 and yearsbetween \u003c\u003d 3 then 1 else 0 end as re_engaged,\n    -- service cross-sell if they did NOT have an encounter in that service category in the past 3 years and DID have an encounter in a DIFFERENT service category in the past 3 years\n    case when \n    -- did NOT have an encounter in that service category in the past 3 years\n    yearsbetween_sc \u003e 3\n    -- DID have an encounter in a DIFFERENT service category in the past 3 years\n    and yearsbetween_not_sc \u003c\u003d 3\n    then 1 else 0 end as xs_sc,\n    -- facility cross-sell if they did NOT have an encounter at that facility in the past 3 years and DID have an encounter in a DIFFERENT facility in the past 3 years\n    case when \n    -- did NOT have an encounter at that facility in the past 3 years\n    yearsbetween_f \u003e 3\n    -- DID have an encounter at a DIFFERENT facility in the past 3 years\n    and yearsbetween_not_f \u003c\u003d 3\n    then 1 else 0 end as xs_f,\n    case when \n    -- service cross-sell\n    (yearsbetween_sc \u003e 3 and yearsbetween_not_sc \u003c\u003d 3)\n    or \n    -- facility cross-sell\n    (yearsbetween_f \u003e 3 and yearsbetween_not_f \u003c\u003d 3) \n    then 1 else 0 end as xs,\n    -- existing patient if they did have an encounter in the previous 3 years within the same facility or service category\n    case when yearsbetween_f_or_sc \u003c\u003d 3 then 1 else 0 end as existing\n    from encounterYearsBetween\n    \"\"\")\n    encounterLifecycleFlags.cache\n    encounterLifecycleFlags.createOrReplaceTempView(\"encounterLifecycleFlags\")\n\n    return encounterLifecycleFlags\n}\n",
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:23:53 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbuildEncounterLifecycleFlagsFromYearsBetween: (spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498079018704_863973914",
      "id": "20170608-142909_539497739",
      "dateCreated": "Jun 21, 2017 9:03:38 PM",
      "dateStarted": "Jun 21, 2017 9:24:00 PM",
      "dateFinished": "Jun 21, 2017 9:24:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def buildEncounterLifecycleStateFromFlags(spark:SparkSession) :org.apache.spark.sql.DataFrame \u003d {\n\n    val sc \u003d spark.sparkContext\n    val sqlContext \u003d spark.sqlContext\n    import sqlContext.implicits._    \n    \n    val encouterLifecycleState \u003d spark.sql(\"\"\"\n    select \n    encounterid,\n    cur.personId,\n    cur.enc_seq,\n    enc_prev,\n    yearsBetween,\n    yearsBetween_f,\n    yearsBetween_sc,\n    yearsBetween_not_sc,\n    yearsBetween_not_f,\n    yearsBetween_f_or_sc,\n    cast(new as Boolean) as new,\n    cast(re_engaged as Boolean) as re_engaged,\n    cast(xs_sc as Boolean) as xs_sc,\n    cast(xs_f as Boolean) as xs_f,\n    cast(xs as Boolean) as xs,\n    cast(existing as Boolean) as existing,\n    \u0027Patient\u0027 as lifecycleTypeHighLevel,\n    -- standard hierarchy: new - cross-service - cross-facility - re-engaged - existing\n    case \n        when new \u003d 1 then \u0027New Patient\u0027\n        when xs_sc \u003d 1 then \u0027Service Cross-Sell\u0027\n        when xs_f \u003d 1 then \u0027Facility Cross-Sell\u0027\n        when re_engaged \u003d 1 then \u0027Re-Engaged Patient\u0027\n        when existing \u003d 1 then \u0027Existing Patient\u0027\n        else \u0027ZZUNKNOWN\u0027 \n    end as lifecycleType,\n    -- service-specific hierarchy: new - cross-service - cross - re-engaged - existing\n    case \n        when new \u003d 1 then \u0027New Patient\u0027\n        when xs_sc \u003d 1 then \u0027Service Cross-Sell\u0027\n        when xs \u003d 1 then \u0027Cross-Sell\u0027\n        when re_engaged \u003d 1 then \u0027Re-Engaged Patient\u0027\n        when existing \u003d 1 then \u0027Existing Patient\u0027\n        else \u0027ZZUNKNOWN\u0027 \n    end as lifecycleType_Service,\n    -- facility-specific hierarchy: new - cross-facility - cross - re-engaged - existing\n    case \n        when new \u003d 1 then \u0027New Patient\u0027\n        when xs_f \u003d 1 then \u0027Facility Cross-Sell\u0027\n        when xs \u003d 1 then \u0027Cross-Sell\u0027\n        when re_engaged \u003d 1 then \u0027Re-Engaged Patient\u0027\n        when existing \u003d 1 then \u0027Existing Patient\u0027\n        else \u0027ZZUNKNOWN\u0027 \n    end as lifecycleType_Facility,\n    case when prv.previousLifecycleType is not null then \u0027Patient\u0027 else \u0027Prospect\u0027 end as previousLifecycleTypeHighLevel,\n    coalesce(prv.previousLifecycleType,\u0027Prospect\u0027) as previousLifecycleType,\n    coalesce(prv.previousLifecycleType_Service,\u0027Prospect\u0027) as previousLifecycleType_Service,\n    coalesce(prv.previousLifecycleType_Facility,\u0027Prospect\u0027) as previousLifecycleType_Facility\n    from encounterLifecycleFlags cur \n    left join (\n    select personid,enc_seq as prv_enc_seq,\n    -- standard hierarchy: new - cross-service - cross-facility - re-engaged - existing\n    case \n        when new \u003d 1 then \u0027New Patient\u0027\n        when xs_sc \u003d 1 then \u0027Service Cross-Sell\u0027\n        when xs_f \u003d 1 then \u0027Facility Cross-Sell\u0027\n        when re_engaged \u003d 1 then \u0027Re-Engaged Patient\u0027\n        when existing \u003d 1 then \u0027Existing Patient\u0027\n        else \u0027ZZUNKNOWN\u0027 \n    end as previousLifecycleType,\n    -- service-specific hierarchy: new - cross-service - cross - re-engaged - existing\n    case \n        when new \u003d 1 then \u0027New Patient\u0027\n        when xs_sc \u003d 1 then \u0027Service Cross-Sell\u0027\n        when xs \u003d 1 then \u0027Cross-Sell\u0027\n        when re_engaged \u003d 1 then \u0027Re-Engaged Patient\u0027\n        when existing \u003d 1 then \u0027Existing Patient\u0027\n        else \u0027ZZUNKNOWN\u0027 \n    end as previousLifecycleType_Service,\n    -- facility-specific hierarchy: new - cross-facility - cross - re-engaged - existing\n    case \n        when new \u003d 1 then \u0027New Patient\u0027\n        when xs_f \u003d 1 then \u0027Facility Cross-Sell\u0027\n        when xs \u003d 1 then \u0027Cross-Sell\u0027\n        when re_engaged \u003d 1 then \u0027Re-Engaged Patient\u0027\n        when existing \u003d 1 then \u0027Existing Patient\u0027\n        else \u0027ZZUNKNOWN\u0027 \n    end as previousLifecycleType_Facility\n    from encounterLifecycleFlags\n    ) prv on prv.personId \u003d cur.personId and prv.prv_enc_seq \u003d cur.enc_prev\n    \"\"\")\n    encouterLifecycleState.cache\n    encouterLifecycleState.createOrReplaceTempView(\"encouterLifecycleState\")   \n\n    return encouterLifecycleState\n}\n",
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:23:53 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbuildEncounterLifecycleStateFromFlags: (spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498079018705_863589165",
      "id": "20170608-200003_90685421",
      "dateCreated": "Jun 21, 2017 9:03:38 PM",
      "dateStarted": "Jun 21, 2017 9:24:00 PM",
      "dateFinished": "Jun 21, 2017 9:24:01 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\n\ncase class state_flags (\n    cross_sell: Boolean,\n    cross_sell_facility: Boolean,\n    cross_sell_service_category: Boolean,\n    existing: Boolean,\n    `new`: Boolean,\n    re_engaged: Boolean\n    )\ncase class previous_state (\n    high_level: String,\n    specific: String\n    )\ncase class current_state (\n    high_level: String,\n    specific: String\n    )\ncase class previous_state_facility_specific (\n    high_level: String,\n    specific: String\n    )\ncase class current_state_facility_specific (\n    high_level: String,\n    specific: String\n    )\n\ncase class previous_state_service_specific (\n    high_level: String,\n    specific: String\n    )\ncase class current_state_service_specific (\n    high_level: String,\n    specific: String\n    )\ncase class patient_lifecycle (\n    current_state: current_state,\n    previous_state: previous_state,\n    current_state_facility_specific: current_state_facility_specific,\n    previous_state_facility_specific: previous_state_facility_specific,\n    current_state_service_specific: current_state_service_specific,\n    previous_state_service_specific: previous_state_service_specific,\n    state_flags: state_flags\n    )\n\ndef writePatientLifecycleDataToEncounter(spark:SparkSession,encounterLifecycleFrame:org.apache.spark.sql.DataFrame,elasticSearchIndexName:String,esServer:String) \u003d {\n            \n        val sc \u003d spark.sparkContext\n        val sqlContext \u003d spark.sqlContext\n        import sqlContext.implicits._    \n\n        val updates \u003d encounterLifecycleFrame\n          .rdd\n          .map(e \u003d\u003e (\n              Map(\n                  ID -\u003e e.getAs[String](\"encounterid\"),\n                  PARENT -\u003e e.getAs[String](\"personId\"),\n                  ROUTING -\u003e e.getAs[String](\"personId\")\n              ),\n              Map(patient_lifecycle -\u003e patient_lifecycle(\n                current_state(e.getAs[String](\"lifecycleTypeHighLevel\"),e.getAs[String](\"lifecycleType\")),\n                previous_state(e.getAs[String](\"previousLifecycleTypeHighLevel\"),e.getAs[String](\"previousLifecycleType\")),\n                current_state_facility_specific(e.getAs[String](\"lifecycleTypeHighLevel\"),e.getAs[String](\"lifecycleType_Facility\")),\n                previous_state_facility_specific(e.getAs[String](\"previousLifecycleTypeHighLevel\"),e.getAs[String](\"previousLifecycleType_Facility\")),\n                current_state_service_specific(e.getAs[String](\"lifecycleTypeHighLevel\"),e.getAs[String](\"lifecycleType_Service\")),\n                previous_state_service_specific(e.getAs[String](\"previousLifecycleTypeHighLevel\"),e.getAs[String](\"previousLifecycleType_Service\")),\n                state_flags(e.getAs[Boolean](\"xs\"),e.getAs[Boolean](\"xs_f\"),e.getAs[Boolean](\"xs_sc\"),e.getAs[Boolean](\"existing\"),e.getAs[Boolean](\"new\"),e.getAs[Boolean](\"re_engaged\"))\n              )\n              )\n            )\n          )\n                    \n        var esconf \u003d Map(\"es.nodes\" -\u003e esServer, \"es.write.operation\" -\u003e \"upsert\", \"es.nodes.wan.only\" -\u003e \"true\",\"es.index.auto.create\" -\u003e \"false\" ,\"es.batch.size.bytes\" -\u003e \"20mb\")\n\n        // write elasticsearch data back to the index\n        updates.saveToEsWithMeta(s\"${elasticSearchIndexName}/encounter\", esconf)\n    }\n",
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:30:48 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\ndefined class state_flags\n\ndefined class previous_state\n\ndefined class current_state\n\ndefined class previous_state_facility_specific\n\ndefined class current_state_facility_specific\n\ndefined class previous_state_service_specific\n\ndefined class current_state_service_specific\n\ndefined class patient_lifecycle\n\nwritePatientLifecycleDataToEncounter: (spark: org.apache.spark.sql.SparkSession, encounterLifecycleFrame: org.apache.spark.sql.DataFrame, elasticSearchIndexName: String, esServer: String)Unit\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498079018706_864743412",
      "id": "20170505-005855_1608577395",
      "dateCreated": "Jun 21, 2017 9:03:38 PM",
      "dateStarted": "Jun 21, 2017 9:30:48 PM",
      "dateFinished": "Jun 21, 2017 9:30:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var iterationCount:Int \u003d 1\nvar lastCount:Int \u003d 0\nvar personIds:String \u003d \"\"\n\n{val (personList,personCount) \u003d\ngetPersonsWithoutEncounterLifecycle(clientCode,esIndexName,esHost,esPort,resultSize)\nlastCount \u003d personCount\npersonIds \u003d personList}\nval encounterSourceFrame \u003d getEncounterSourceData(spark,clientCode,esIndexName,esHost,personIds)\nval encounterBaseFrame \u003d buildEncounterBaseFromSource(spark)\nval encounterYearsBetweenFrame \u003d buildEncounterYearsBetweenFromBase(spark)\nval encounterLifecycleFlagsFrame \u003d buildEncounterLifecycleFlagsFromYearsBetween(spark)\nval encounterLifecycleStateFrame \u003d buildEncounterLifecycleStateFromFlags(spark)\n\nwritePatientLifecycleDataToEncounter(spark,encounterLifecycleStateFrame,esIndexName,esHost)\n",
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:29:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\niterationCount: Int \u003d 1\n\nlastCount: Int \u003d 0\n\npersonIds: String \u003d \"\"\nidlist length 10000\nreturned 10000 results\n\nencounterSourceFrame: org.apache.spark.sql.DataFrame \u003d [admit_date: timestamp, facility: struct\u003ccode: string, name: string\u003e ... 4 more fields]\n\nencounterBaseFrame: org.apache.spark.sql.DataFrame \u003d [encounterId: string, personId: string ... 9 more fields]\n\nencounterYearsBetweenFrame: org.apache.spark.sql.DataFrame \u003d [encounterId: string, personId: string ... 8 more fields]\n\nencounterLifecycleFlagsFrame: org.apache.spark.sql.DataFrame \u003d [encounterid: string, personId: string ... 14 more fields]\n\nencounterLifecycleStateFrame: org.apache.spark.sql.DataFrame \u003d [encounterid: string, personId: string ... 22 more fields]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 61.0 failed 1 times, most recent failure: Lost task 4.0 in stage 61.0 (TID 7863, localhost, executor driver): java.lang.AssertionError: assertion failed\n\tat scala.Predef$.assert(Predef.scala:156)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scalaSimpleName(JavaMirrors.scala:966)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:988)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$methodToScala1(JavaMirrors.scala:898)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$methodToScala$1.apply(JavaMirrors.scala:894)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$methodToScala$1.apply(JavaMirrors.scala:894)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.methodToScala(JavaMirrors.scala:894)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:847)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n\tat scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n\tat scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n\tat scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classSymbol(JavaMirrors.scala:196)\n\tat scala.reflect.runtime.JavaMirrors$JavaMirror.classSymbol(JavaMirrors.scala:54)\n\tat org.elasticsearch.spark.serialization.ReflectionUtils$.org$elasticsearch$spark$serialization$ReflectionUtils$$checkCaseClass(ReflectionUtils.scala:42)\n\tat org.elasticsearch.spark.serialization.ReflectionUtils$$anonfun$checkCaseClassCache$1.apply(ReflectionUtils.scala:84)\n\tat org.elasticsearch.spark.serialization.ReflectionUtils$$anonfun$checkCaseClassCache$1.apply(ReflectionUtils.scala:83)\n\tat scala.collection.mutable.MapLike$class.getOrElseUpdate(MapLike.scala:194)\n\tat scala.collection.mutable.AbstractMap.getOrElseUpdate(Map.scala:80)\n\tat org.elasticsearch.spark.serialization.ReflectionUtils$.checkCaseClassCache(ReflectionUtils.scala:83)\n\tat org.elasticsearch.spark.serialization.ReflectionUtils$.isCaseClass(ReflectionUtils.scala:102)\n\tat org.elasticsearch.spark.serialization.ScalaValueWriter.org$elasticsearch$spark$serialization$ScalaValueWriter$$doWrite(ScalaValueWriter.scala:90)\n\tat org.elasticsearch.spark.serialization.ScalaValueWriter$$anonfun$org$elasticsearch$spark$serialization$ScalaValueWriter$$doWrite$2.apply(ScalaValueWriter.scala:53)\n\tat org.elasticsearch.spark.serialization.ScalaValueWriter$$anonfun$org$elasticsearch$spark$serialization$ScalaValueWriter$$doWrite$2.apply(ScalaValueWriter.scala:50)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.immutable.Map$Map1.foreach(Map.scala:116)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.elasticsearch.spark.serialization.ScalaValueWriter.org$elasticsearch$spark$serialization$ScalaValueWriter$$doWrite(ScalaValueWriter.scala:50)\n\tat org.elasticsearch.spark.serialization.ScalaValueWriter.write(ScalaValueWriter.scala:37)\n\tat org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:53)\n\tat org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71)\n\tat org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8)\n\tat org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:159)\n\tat org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:67)\n\tat org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102)\n\tat org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n  at org.elasticsearch.spark.rdd.EsSpark$.doSaveToEs(EsSpark.scala:102)\n  at org.elasticsearch.spark.rdd.EsSpark$.saveToEsWithMeta(EsSpark.scala:85)\n  at org.elasticsearch.spark.rdd.EsSpark$.saveToEsWithMeta(EsSpark.scala:82)\n  at org.elasticsearch.spark.package$SparkPairRDDFunctions.saveToEsWithMeta(package.scala:73)\n  at $$$$bec6d1991b88c272b3efac29d720f546$$$writePatientLifecycleDataToEncounter(\u003cconsole\u003e:183)\n  ... 78 elided\nCaused by: java.lang.AssertionError: assertion failed\n  at scala.Predef$.assert(Predef.scala:156)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scalaSimpleName(JavaMirrors.scala:966)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:988)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classTo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:849)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$methodToScala1(JavaMirrors.scala:898)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$methodToScala$1.apply(JavaMirrors.scala:894)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$methodToScala$1.apply(JavaMirrors.scala:894)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.methodToScala(JavaMirrors.scala:894)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.sOwner(JavaMirrors.scala:847)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.scala$reflect$runtime$JavaMirrors$JavaMirror$$classToScala1(JavaMirrors.scala:987)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror$$anonfun$classToScala$1.apply(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$J\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\navaMirror$$anonfun$toScala$1.apply(JavaMirrors.scala:97)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache$$anonfun$toScala$1.apply(TwoWayCaches.scala:38)\n  at scala.reflect.runtime.Gil$class.gilSynchronized(Gil.scala:19)\n  at scala.reflect.runtime.JavaUniverse.gilSynchronized(JavaUniverse.scala:16)\n  at scala.reflect.runtime.TwoWayCaches$TwoWayCache.toScala(TwoWayCaches.scala:33)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.toScala(JavaMirrors.scala:95)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classToScala(JavaMirrors.scala:980)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classSymbol(JavaMirrors.scala:196)\n  at scala.reflect.runtime.JavaMirrors$JavaMirror.classSymbol(JavaMirrors.scala:54)\n  at org.elasticsearch.spark.serialization.ReflectionUtils$.org$elasticsearch$spark$serialization$ReflectionUtils$$checkCaseClass(ReflectionUtils.scala:42)\n  at org.elasticsearch.spark.serialization.ReflectionUtils$$anonfun$checkCaseClassCache$1.apply(ReflectionUtils.scala:84)\n  at org.elasticsearch.spark.serialization.ReflectionUtils$$anonfun$checkCaseClassCache$1.apply(ReflectionUtils.scala:83)\n  at scala.collection.mutable.MapLike$class.getOrElseUpdate(MapLike.scala:194)\n  at scala.collection.mutable.AbstractMap.getOrElseUpdate(Map.scala:80)\n  at org.elasticsearch.spark.serialization.ReflectionUtils$.checkCaseClassCache(ReflectionUtils.scala:83)\n  at org.elasticsearch.spark.serialization.ReflectionUtils$.isCaseClass(ReflectionUtils.scala:102)\n  at org.elasticsearch.spark.serialization.ScalaValueWriter.org$elasticsearch$spark$serialization$ScalaValueWriter$$doWrite(ScalaValueWriter.scala:90)\n  at org.elasticsearch.spark.serialization.ScalaValueWriter$$anonfun$org$elasticsearch$spark$serialization$ScalaValueWriter$$doWrite$2.apply(ScalaValueWriter.scala:53)\n  at org.elasticsearch.spark.serialization.ScalaValueWriter$$anonfun$org$elasticsearch$spark$serialization$ScalaValueWriter$$doWrite$2.apply(ScalaValueWriter.scala:50)\n  at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n  at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)\n  at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n  at org.elasticsearch.spark.serialization.ScalaValueWriter.org$elasticsearch$spark$serialization$ScalaValueWriter$$doWrite(ScalaValueWriter.scala:50)\n  at org.elasticsearch.spark.serialization.ScalaValueWriter.write(ScalaValueWriter.scala:37)\n  at org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:53)\n  at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71)\n  at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58)\n  at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:159)\n  at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:67)\n  at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102)\n  at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n  ... 3 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498080527954_1159223508",
      "id": "20170621-212847_1635086615",
      "dateCreated": "Jun 21, 2017 9:28:47 PM",
      "dateStarted": "Jun 21, 2017 9:29:01 PM",
      "dateFinished": "Jun 21, 2017 9:30:08 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\nwritePatientLifecycleDataToEncounter(spark,encounterLifecycleStateFrame,esIndexName,esHost)",
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:30:57 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1498080655611_1461969463",
      "id": "20170621-213055_1447128247",
      "dateCreated": "Jun 21, 2017 9:30:55 PM",
      "dateStarted": "Jun 21, 2017 9:30:57 PM",
      "dateFinished": "Jun 21, 2017 9:32:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "var iterationCount:Int \u003d 1\nvar lastCount:Int \u003d 0\nvar personIds:String \u003d \"\"\n\n{val (personList,personCount) \u003d\ngetPersonsWithoutEncounterLifecycle(clientCode,esIndexName,esHost,esPort,resultSize)\nlastCount \u003d personCount\npersonIds \u003d personList}\n\nwhile ( lastCount \u003e 0 ) {\n    println(\"iteration \" + iterationCount.toString + \" person count \" + lastCount.toString);\n\n    iterationCount \u003d iterationCount + 1\n\n    val encounterSourceFrame \u003d getEncounterSourceData(spark,clientCode,esIndexName,esHost,personIds)\n    val encounterBaseFrame \u003d buildEncounterBaseFromSource(spark)\n    val encounterYearsBetweenFrame \u003d buildEncounterYearsBetweenFromBase(spark)\n    val encounterLifecycleFlagsFrame \u003d buildEncounterLifecycleFlagsFromYearsBetween(spark)\n    val encounterLifecycleStateFrame \u003d buildEncounterLifecycleStateFromFlags(spark)\n\n    writePatientLifecycleDataToEncounter(spark,encounterLifecycleStateFrame,esIndexName,esHost)\n\n    encounterSourceFrame.unpersist()\n\n    {val (personList,personCount) \u003d\n    getPersonsWithoutEncounterLifecycle(clientCode,esIndexName,esHost,esPort,resultSize)\n    lastCount \u003d personCount\n    personIds \u003d personList}\n}\n\n",
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:32:54 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\niterationCount: Int \u003d 1\n\nlastCount: Int \u003d 0\n\npersonIds: String \u003d \"\"\nidlist length 10000\nreturned 10000 results\niteration 1 person count 10000\nidlist length 10000\nreturned 10000 results\niteration 2 person count 10000\nidlist length 10000\nreturned 10000 results\niteration 3 person count 10000\nidlist length 10000\nreturned 10000 results\niteration 4 person count 10000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job 46 cancelled part of cancelled job group zeppelin-20170608-191817_1472378738\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:788)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1625)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n  at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:266)\n  at org.apache.spark.RangePartitioner.\u003cinit\u003e(Partitioner.scala:128)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$.prepareShuffleDependency(ShuffleExchange.scala:218)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:84)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:121)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:112)\n  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n  at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:112)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:235)\n  at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:121)\n  at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:42)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:368)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)\n  at org.apache.spark.sql.execution.columnar.InMemoryRelation.buildBuffers(InMemoryRelation.scala:96)\n  at org.apache.spark.sql.execution.columnar.InMemoryRelation.\u003cinit\u003e(InMemoryRelation.scala:85)\n  at org.apache.spark.sql.execution.columnar.InMemoryRelation$.apply(InMemoryRelation.scala:41)\n  at org.apache.spark.sql.execution.CacheManager$$anonfun$cacheQuery$1.apply(CacheManager.scala:98)\n  at org.apache.spark.sql.execution.CacheManager.writeLock(CacheManager.scala:65)\n  at org.apache.spark.sql.execution.CacheManager.cacheQuery(CacheManager.scala:89)\n  at org.apache.spark.sql.Dataset.persist(Dataset.scala:2479)\n  at org.apache.spark.sql.Dataset.cache(Dataset.scala:2489)\n  at $$$$bec6d1991b88c272b3efac29d720f546$$$buildEncounterBaseFromSource(\u003cconsole\u003e:133)\n  ... 78 elided\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498079018707_864358663",
      "id": "20170608-191817_1472378738",
      "dateCreated": "Jun 21, 2017 9:03:38 PM",
      "dateStarted": "Jun 21, 2017 9:32:54 PM",
      "dateFinished": "Jun 21, 2017 9:40:55 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "dateUpdated": "Jun 21, 2017 9:23:53 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1498079018708_862434918",
      "id": "20170609-134859_1144877786",
      "dateCreated": "Jun 21, 2017 9:03:38 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/patient_lifecycle/FINAL/0.2.2 Encounter Patient Lifecycle Processing",
  "id": "2CN8KUA79",
  "angularObjects": {
    "2CK39QPEV:shared_process": [],
    "2CHGDFEAS:shared_process": [],
    "2CJPWUK64:shared_process": [],
    "2CJPRUUCQ:shared_process": [],
    "2CMA6NN69:shared_process": [],
    "2CJM2HZ3E:shared_process": [],
    "2CHUKUNGV:shared_process": [],
    "2CMSNT58Q:shared_process": [],
    "2CMY621JM:shared_process": []
  },
  "config": {},
  "info": {}
}