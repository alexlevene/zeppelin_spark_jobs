{
  "paragraphs": [
    {
      "text": "// dependencies required for elasticsearch direct access via REST client interface\nimport org.apache.http.client.methods.HttpPost\nimport org.apache.http.entity.StringEntity\nimport org.apache.http.impl.client.DefaultHttpClient\nimport org.apache.http.impl.client.BasicResponseHandler\nimport java.util.Date\nimport java.util.Calendar\nimport org.apache.commons.lang3.time.DateUtils\nimport java.text.SimpleDateFormat\nimport scala.collection.mutable.WrappedArray\n\n\n// dependencies\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.SQLContext._\nimport org.apache.spark.sql.functions._\n\nimport org.elasticsearch.spark.sql._\nimport org.elasticsearch.spark._\nimport org.elasticsearch.spark.rdd.Metadata._ \n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\n\nimport spark.implicits._\n\n",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 9:48:21 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nimport org.apache.http.client.methods.HttpPost\n\nimport org.apache.http.entity.StringEntity\n\nimport org.apache.http.impl.client.DefaultHttpClient\n\nimport org.apache.http.impl.client.BasicResponseHandler\n\nimport java.util.Date\n\nimport java.util.Calendar\n\nimport org.apache.commons.lang3.time.DateUtils\n\nimport java.text.SimpleDateFormat\n\nimport scala.collection.mutable.WrappedArray\n\nimport org.apache.spark.sql.SQLContext\n\nimport org.apache.spark.sql.SQLContext._\n\nimport org.apache.spark.sql.functions._\n\nimport org.elasticsearch.spark.sql._\n\nimport org.elasticsearch.spark._\n\nimport org.elasticsearch.spark.rdd.Metadata._\n\nimport org.apache.spark.SparkContext\n\nimport org.apache.spark.SparkContext._\n\nimport spark.implicits._\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497970715598_-1796539105",
      "id": "20170620-145835_1359905521",
      "dateCreated": "Jun 20, 2017 2:58:35 PM",
      "dateStarted": "Jun 20, 2017 9:48:21 PM",
      "dateFinished": "Jun 20, 2017 9:48:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val clientCode \u003d \"DEMO\"\nval esIndexName \u003d \"exp_rjj_1_3\"\nval esPort \u003d 9200\nval esHost \u003d \"elasticsearch.exp-dev.io\"\nval resultLimit \u003d 1000",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 9:48:21 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nclientCode: String \u003d DEMO\n\nesIndexName: String \u003d exp_rjj_1_3\n\nesPort: Int \u003d 9200\n\nesHost: String \u003d elasticsearch.exp-dev.io\n\nresultLimit: Int \u003d 1000\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497970773303_719577495",
      "id": "20170620-145933_538760620",
      "dateCreated": "Jun 20, 2017 2:59:33 PM",
      "dateStarted": "Jun 20, 2017 9:48:24 PM",
      "dateFinished": "Jun 20, 2017 9:48:34 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def getHouseholdsWithoutRetention(clientCode:String,esIndexName:String,esHost:String,esPort:Int,resultLimit:Int \u003d 10000) :(String,Int) \u003d {\n    val householdsWithoutRetention \u003d s\"\"\"\n    {\n      \"size\": 0,\n      \"_source\": [\"household.household_number\"],\n      \"query\": {\n        \"constant_score\": {\n          \"filter\": {\n            \"bool\": {\n              \"must\": [\n                {\"term\":{\"client_code\" : \"${clientCode}\" }},\n                {\n                  \"has_child\": {\n                    \"child_type\": \"encounter\",\n                    \"query\": {\n                      \"bool\": {\n                        \"must\": [\n                          { \"term\": {\"client_code\": \"${clientCode}\" }},\n                          { \"range\": {\"admit_date\": {\"gte\": \"now-4y\", \"lte\": \"now\"}}}\n                        ]\n                      }\n                    }\n                  }\n                },\n                {\"exists\": { \"field\": \"household.household_number\"} }\n              ],\n              \"must_not\": [\n                {\n                  \"nested\": {\n                    \"path\": \"household.household_retention_history\",\n                    \"query\": {\n                      \"exists\": { \"field\": \"household.household_retention_history.retained\"}\n                    }\n                  }\n                }\n              ]\n            }\n          }\n        }\n      },\n      \"aggs\": {\n        \"all_households\": {\n          \"terms\": {\n            \"field\": \"household.household_number\",\n            \"size\": ${resultLimit},\n            \"order\": {\"_term\":\"asc\"}\n          }\n        }\n      }\n    }\n    \"\"\"\n\n    // generate the elasticsearch request string\n    \n    val endPoint \u003d \"/\"+ esIndexName + \"/person/_search\"\n    val request_url \u003d \"http://\" + esHost + \":\" + esPort + endPoint\n    \n    // build the apache HTTP post request\n    val post \u003d new HttpPost(request_url)\n    // set header for json request string\n    val setHeaderReturn \u003d post.setHeader(\"Content-type\",\"application/json\")\n    // ad the json request string to the http post\n    val setEntityReturn \u003d post.setEntity(new StringEntity(householdsWithoutRetention))\n    // send the request to elasticsearch\n    val response \u003d (new DefaultHttpClient).execute(post)\n    // get the status -- this code doesn\u0027t check for HTTP/1.1 200 OK response but the final code should!\n    val status \u003d response.getStatusLine\n    // get the json response body\n    val responseBody \u003d (new BasicResponseHandler).handleResponse(response).trim.toString\n    \n    // use a spark RDD to parse json response body\n    // val vals \u003d sc.parallelize(responseBody ::  Nil)\n    // query RDD to extract relevant json key (aggregations.min_admit.value)\n    val df \u003d sqlContext.read.json(sc.parallelize(responseBody::Nil)).select($\"aggregations.all_households.buckets.key\")\n    \n    val idList \u003d df.first().getAs[WrappedArray[String]](0)\n    \n    val idlist_length \u003d idList.length\n    println(s\"idlist length ${idlist_length}\")\n    \n    if (idlist_length \u003d\u003d 0) {\n        return (null,idlist_length)\n    }\n    \n    val idString \u003d idList.mkString(\"\\\"\",\"\\\",\\n\\\"\",\"\\\"\")\n    \n    println (s\"returned ${resultLimit} results\")\n\n    return (idString, idlist_length)\n}",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 9:48:21 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\ngetHouseholdsWithoutRetention: (clientCode: String, esIndexName: String, esHost: String, esPort: Int, resultLimit: Int)(String, Int)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497971191808_2054485967",
      "id": "20170620-150631_1218324604",
      "dateCreated": "Jun 20, 2017 3:06:31 PM",
      "dateStarted": "Jun 20, 2017 9:48:33 PM",
      "dateFinished": "Jun 20, 2017 9:48:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def getEncounterSourceData(clientCode:String,esIndexName:String,esHost:String,esPort:Int,householdList:String) :org.apache.spark.sql.DataFrame \u003d {\n\n    val encounterSourceQuery \u003d s\"\"\"\n{\n  \"_source\": [\"encounter_key\",\"parent\",\"admit_date\"],\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"bool\": {\n          \"must\": [\n            {\"term\":{\"client_code\" : \"${clientCode}\" }},\n            { \"range\": {\"admit_date\": {\"gte\": \"now-5y\", \"lte\": \"now\"}}},\n            {\n              \"has_parent\": {\n                \"parent_type\": \"person\",\n                \"query\": {\n                  \"bool\": {\n                    \"must\": [\n                      { \"term\": {\"client_code\": \"${clientCode}\" }},\n                      {\"terms\": {\"household.household_number\": [\n${householdList}\n                        ]\n                      }}\n                    ]\n                  }\n                }\n              } \n            }\n          ]\n        }\n      }\n    }\n  }\n}\n    \"\"\"\n\n    \n    // limit the fields that are included\n    val encounterSourceQueryOptions \u003d Map(\n    //  \"es.read.field.include\" -\u003e \"encounter_key,parent,admit_date\"\n      \"es.read.field.exclude\" -\u003e \"personKey,person_key,recordId,admit_age,service_category,facility,patient_lifecycle,admit_source,admit_type,client_code,client_name,discharge_date,encounter_type,financial_class,msdrg,service_sub_category,diagnosis,cpt,procedure,campaigns,total_charges,total_amount_received,expected_reimbursement,direct_costs,actual_contribution_margin,expected_contribution_margin,recency_frequency\"\n      )\n    \n    val encountersES \u003d sqlContext.esDF(s\"${esIndexName}/encounter\", encounterSourceQuery, encounterSourceQueryOptions)\n    encountersES.cache\n    encountersES.createOrReplaceTempView(\"encountersES\")\n    return encountersES\n}\n",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 9:48:22 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\ngetEncounterSourceData: (clientCode: String, esIndexName: String, esHost: String, esPort: Int, householdList: String)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497972312197_1613585630",
      "id": "20170620-152512_204215003",
      "dateCreated": "Jun 20, 2017 3:25:12 PM",
      "dateStarted": "Jun 20, 2017 9:48:34 PM",
      "dateFinished": "Jun 20, 2017 9:48:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def getPersonSourceData(clientCode:String,esIndexName:String,esHost:String,esPort:Int,householdList:String) :org.apache.spark.sql.DataFrame \u003d {\n\n    val personSourceQuery \u003d s\"\"\"\n{\n  \"_source\": [\"_recordId\",\"household.household_number\"],\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"bool\": {\n          \"must\": [\n            {\"term\":{\"client_code\" : \"DEMO\" }},\n            {\"terms\": {\"household.household_number\": [\n${householdList}\n              ]\n            }}\n          ]\n        }\n      }\n    }\n  }\n}\n    \"\"\"\n    // limit the fields that are included\n    val personSourceQueryOptions \u003d Map(\n    //  \"es.read.field.include\" -\u003e \"_recordId,household.household_number\"\n      \"es.read.field.exclude\" -\u003e \"person_key,address,birth_date,children_present,client_code,client_name,communication,email,ethnicity,first_name,gender,language,last_name,marital_status,middle_initial,middle_name,mobile_phone,payor_category,payor_category_confidence,prefix,race,religion,suffix,campaigns,recency_frequency,perceptual_profile,chui,pdi,patient_lifecycle_history,deceased_date\"\n      )\n    \n    val personsES \u003d sqlContext.esDF(s\"${esIndexName}/person\", personSourceQuery, personSourceQueryOptions)\n    personsES.cache\n    personsES.createOrReplaceTempView(\"personsES\")\n    return personsES\n}\n",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 9:48:22 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\ngetPersonSourceData: (clientCode: String, esIndexName: String, esHost: String, esPort: Int, householdList: String)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497972494598_-1354723130",
      "id": "20170620-152814_1878905727",
      "dateCreated": "Jun 20, 2017 3:28:14 PM",
      "dateStarted": "Jun 20, 2017 9:48:35 PM",
      "dateFinished": "Jun 20, 2017 9:48:36 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def getRetentionMonthRange() :org.apache.spark.sql.DataFrame \u003d {\n\n    def monthIterator(start: org.joda.time.LocalDate, end: org.joda.time.LocalDate) \u003d Iterator.iterate(start)(_ plusMonths 1) takeWhile (_ isBefore end)\n\n    val max_admit_month \u003d (new org.joda.time.LocalDate).withDayOfMonth(1)\n    val min_admit_month \u003d max_admit_month.plusYears(-4)\n\n    val stringMonthIterator \u003d monthIterator(min_admit_month,max_admit_month.plusMonths(1)).map(x \u003d\u003e x.toString())\n\n    val stringMonthList \u003d stringMonthIterator.toList\n\n    val monthRangeFrame \u003d spark.createDataFrame(stringMonthList.map(Tuple1(_))).toDF(\"startOfMonth\")\n\n    monthRangeFrame.createOrReplaceTempView(\"retentionMonthRange\")\n    return monthRangeFrame\n}\n",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 9:48:22 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nwarning: Class org.joda.convert.FromString not found - continuing with a stub.\n\nwarning: Class org.joda.convert.ToString not found - continuing with a stub.\n\nwarning: Class org.joda.convert.FromString not found - continuing with a stub.\n\nwarning: Class org.joda.convert.ToString not found - continuing with a stub.\n\ngetRetentionMonthRange: ()org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497973425591_979467416",
      "id": "20170620-154345_1069029982",
      "dateCreated": "Jun 20, 2017 3:43:45 PM",
      "dateStarted": "Jun 20, 2017 9:48:35 PM",
      "dateFinished": "Jun 20, 2017 9:48:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def buildHouseholdRetentionBase(personView:String \u003d \"personsES\", encounterView:String \u003d \"encountersES\",retentionMonthView:String \u003d \"retentionMonthRange\") :org.apache.spark.sql.DataFrame \u003d {\n\n    // ADD THE PROSPECT RECORDS AT THE BEGINNING OF THE LIFECYCLE CHAIN FOR STANDARD TYPE\n    // add the stub records at the end of the lifecycle chain to track end-dates\n    val householdRetentionBase \u003d spark.sql(s\"\"\"\n        with\n        cur as (\n            select\n            household.household_number as household,\n            e.admit_date,\n            date_add(e.admit_date,-365.25) as admit_date_minus1\n            from ${personView} p join ${encounterView} e on e.parent \u003d p._recordId\n            group by household.household_number,e.admit_date,date_add(e.admit_date,-365.25)\n        ),\n        all as (\n            select \n            p.household.household_number as household,\n            d.startOfMonth,\n            0 as isRetained\n            from ${personView} p cross join ${retentionMonthView} d \n            group by p.household.household_number,d.startOfMonth\n        ),\n        retained as (\n            select cur.household,\n            trunc(cur.admit_date,\u0027MM\u0027) as admit_month,\n            1 as isRetained\n            from cur join cur as prv\n            on cur.household \u003d prv.household\n            and cur.admit_date \u003e prv.admit_date\n            and prv.admit_date \u003e\u003d cur.admit_date_minus1\n            group by cur.household,trunc(cur.admit_date,\u0027MM\u0027)\n        )\n        select\n        c.household,\n        row_number() over (partition by c.household order by c.startOfMonth) as month_rank,\n        c.startOfMonth,\n        coalesce(r.isRetained,c.isRetained) as isRetained\n        from all c \n        left join retained r on c.household \u003d r.household and c.startOfMonth \u003d r.admit_month\n    \"\"\")\n    householdRetentionBase.cache\n    householdRetentionBase.createOrReplaceTempView(\"householdRetentionBase\")\n    // encounterYearsBetween.show()\n\n    return householdRetentionBase\n}",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 9:48:22 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbuildHouseholdRetentionBase: (personView: String, encounterView: String, retentionMonthView: String)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497984151860_-297226387",
      "id": "20170620-184231_143535807",
      "dateCreated": "Jun 20, 2017 6:42:31 PM",
      "dateStarted": "Jun 20, 2017 9:48:36 PM",
      "dateFinished": "Jun 20, 2017 9:48:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def buildHouseholdRetentionCollapsed(householdRetentionBaseView:String \u003d \"householdRetentionBase\") :org.apache.spark.sql.DataFrame \u003d {\n\n    val max_admit_month \u003d (new org.joda.time.LocalDate).withDayOfMonth(1)\n    val householdRetentionCollapsed \u003d spark.sql(s\"\"\"\n        with retentionRanked as (\n            select \n            household,\n            month_rank,\n            startOfMonth,\n            cast(isRetained as boolean) as isRetained,\n            case when lag(isRetained,1,0) over (partition by household order by month_rank) \u003d isRetained and month_rank \u003c\u003e 1 then 1 else 0 end as same_state\n            from ${householdRetentionBaseView}\n        )\n        select\n        r.household,\n        r.month_rank,\n        date_format(r.startOfMonth,\u0027yyyy-MM-dd\u0027) as start_date,\n        date_format(from_unixtime(unix_timestamp(add_months(coalesce(z.end_of_range,r.startOfMonth),1)) - 1),\u0027yyyy-MM-dd HH:mm:ss\u0027) as end_date,\n        unix_timestamp(trunc(r.startOfMonth,\u0027MM\u0027)) * 1000 as start_date_epoch,\n        (unix_timestamp(add_months(coalesce(z.end_of_range,r.startOfMonth),1)) - 1) * 1000 as end_date_epoch,\n        r.isRetained\n        from retentionRanked r \n        -- all rows at beginning of chain \n        left join (select household, month_rank,startOfMonth from (select household,startOfMonth,month_rank,same_state, lead(same_state) over (partition by household order by month_rank) as next_state from retentionRanked) where same_state \u003d 0 and next_state \u003d 1) c on c.household \u003d r.household and c.month_rank \u003d r.month_rank\n        left join (\n            select c.household,c.month_rank,c.startOfMonth, min(n.month_rank) as next_month_rank,\n            min(case when n.startOfMonth \u003d\u0027${max_admit_month}\u0027 then n.startOfMonth else  n.previous_startOfMonth end) as end_of_range\n            from\n            (select household, month_rank,startOfMonth from (select household,startOfMonth,month_rank,same_state, lead(same_state) over (partition by household order by month_rank) as next_state from retentionRanked) where same_state \u003d 0 and next_state \u003d 1) c\n            -- all rows not in same_state\n            join (select household,month_rank,same_state\n            ,lag(startOfMonth) over (partition by household order by month_rank) as previous_startOfMonth,startOfMonth from retentionRanked) n on n.household \u003d c.household and c.month_rank \u003c n.month_rank \n            and (n.same_state \u003d 0 or n.startOfMonth \u003d \u0027${max_admit_month}\u0027)\n            group by c.household,c.month_rank,c.startOfMonth\n        ) z on r.household \u003d z.household and r.month_rank \u003d z.month_rank\n        where r.same_state \u003d 0\n    \"\"\")\n    householdRetentionCollapsed.cache\n    householdRetentionCollapsed.createOrReplaceTempView(\"householdRetentionCollapsed\")\n    // encounterYearsBetween.show()\n\n    return householdRetentionCollapsed\n}",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 9:48:22 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbuildHouseholdRetentionCollapsed: (householdRetentionBaseView: String)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497992693710_1550671372",
      "id": "20170620-210453_937126773",
      "dateCreated": "Jun 20, 2017 9:04:53 PM",
      "dateStarted": "Jun 20, 2017 9:48:38 PM",
      "dateFinished": "Jun 20, 2017 9:48:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def buildHouseholdRetentionFinal(personView:String \u003d \"personsES\", householdRetentionCollapsedView:String \u003d \"householdRetentionCollapsed\") :org.apache.spark.sql.DataFrame \u003d {\n\n    val householdRetentionFinal \u003d spark.sql(s\"\"\"\n        with personToHousehold as (\n            select\n            _recordId as person_key,\n            cast(household.household_income_range.minimum as Long) as income_min,\n            cast(household.household_income_range.maximum as Long) as income_max,\n            household.household_number as household\n            from ${personView}\n        )\n        select\n        p.person_key,\n        p.income_min,\n        p.income_max,\n        p.household,\n        r.start_date,\n        r.end_date,\n        r.start_date_epoch,\n        r.end_date_epoch,\n        r.isRetained\n        from ${householdRetentionCollapsedView} r join personToHousehold p on p.household \u003d r.household\n    \"\"\")\n    householdRetentionFinal.cache\n    householdRetentionFinal.createOrReplaceTempView(\"householdRetentionFinal\")\n    // encounterYearsBetween.show()\n\n    return householdRetentionFinal\n}",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 10:48:53 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbuildHouseholdRetentionFinal: (personView: String, householdRetentionCollapsedView: String)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497994325104_1697193351",
      "id": "20170620-213205_11688440",
      "dateCreated": "Jun 20, 2017 9:32:05 PM",
      "dateStarted": "Jun 20, 2017 10:48:53 PM",
      "dateFinished": "Jun 20, 2017 10:48:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// build combined patient lifecycle data set\ndef writeHouseholdRetentionDataToPerson(householdRetentionFrame:org.apache.spark.sql.DataFrame,elasticSearchIndexName:String) \u003d {\n        \n    \n    case class date_range (\n        gte: Long,\n        lte: Long\n        )\n    case class date_range_alt (\n        gte: String,\n        lte: String\n        )\n    case class previous_state (\n        high_level: String,\n        specific: String\n        )\n    case class household_retention_history (\n        date_range: date_range,\n        date_range_alt: date_range_alt,\n        retained: Boolean\n        )\n    \n    val updates \u003d householdRetentionFrame.where(col(\u0027income_min\u0027) !\u003d \u0027null\u0027)\n      .rdd\n      .groupBy( z \u003d\u003e z.getAs[String](\"person_key\"))\n      .map(e \u003d\u003e (\n            Map(ID -\u003e e._1),\n            Map(household_retention_history -\u003e e._2.map(a \u003d\u003e household_retention_history(\n            date_range(a.getAs[Long](\"start_date_epoch\"), a.getAs[Long](\"end_date_epoch\")),\n            date_range_alt(a.getAs[String](\"start_date\"), a.getAs[String](\"end_date\")),\n            a.getAs[Boolean](\"isRetained\")\n          )\n          ).toArray)\n        ))\n \n    // write elasticsearch data back to the index\n    updates.saveToEsWithMeta(s\"${elasticSearchIndexName}/person\", Map(\"es.batch.size.bytes\" -\u003e \"20mb\"))\n\n}\n",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 10:56:24 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nwriteHouseholdRetentionDataToPerson: (householdRetentionFrame: org.apache.spark.sql.DataFrame, elasticSearchIndexName: String)Unit\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497994765246_-889264423",
      "id": "20170620-213925_459246789",
      "dateCreated": "Jun 20, 2017 9:39:25 PM",
      "dateStarted": "Jun 20, 2017 9:48:39 PM",
      "dateFinished": "Jun 20, 2017 9:48:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// suppress the output of this command with curlies\n{val (householdList,householdCount) \u003d getHouseholdsWithoutRetention(clientCode,esIndexName,esHost,esPort,resultLimit)\n\nval encountersES \u003d getEncounterSourceData(clientCode,esIndexName,esHost,esPort,householdList)\n\nval personsES \u003d getPersonSourceData(clientCode,esIndexName,esHost,esPort,householdList)\n\nval monthRangeFrame \u003d getRetentionMonthRange()\n\nval householdRetentionBase \u003d buildHouseholdRetentionBase()\n\nval householdRetentionCollapsed \u003d buildHouseholdRetentionCollapsed()\n\n\n// writeHouseholdRetentionDataToPerson(householdRetentionFinal,esIndexName)\n}",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 10:15:53 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "idlist length 1000\nreturned 1000 results\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497990586245_1077981371",
      "id": "20170620-202946_1380390546",
      "dateCreated": "Jun 20, 2017 8:29:46 PM",
      "dateStarted": "Jun 20, 2017 10:15:53 PM",
      "dateFinished": "Jun 20, 2017 10:15:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val householdRetentionFrame \u003d buildHouseholdRetentionFinal()\n",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 10:49:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nhouseholdRetentionFrame: org.apache.spark.sql.DataFrame \u003d [person_key: string, income_min: bigint ... 7 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497996964196_1105415606",
      "id": "20170620-221604_113283957",
      "dateCreated": "Jun 20, 2017 10:16:04 PM",
      "dateStarted": "Jun 20, 2017 10:49:01 PM",
      "dateFinished": "Jun 20, 2017 10:49:02 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "      \n    \n    case class date_range (\n        gte: Long,\n        lte: Long\n        )\n    case class date_range_alt (\n        gte: String,\n        lte: String\n        )\n    case class household_income_range (\n        minimum: Long,\n        maximum: Long\n        )\n    case class household_retention_history (\n        date_range: date_range,\n        date_range_alt: date_range_alt,\n        retained: Boolean\n        )\n    case class household (\n        household_number: String,\n        household_income_range: household_income_range,\n        household_retention_history: Iterable[household_retention_history]\n        )\n\n    case class household_no_income_range (\n        household_number: String,\n        household_retention_history: Iterable[household_retention_history]\n        )\n\n\n\n    val updates_no_income_range \u003d householdRetentionFrame.filter(\"income_min is null\")\n      .rdd\n      .groupBy( z \u003d\u003e z.getAs[String](\"person_key\") + \"|\" + z.getAs[String](\"household\") + \"|\" + z.getAs[String](\"income_min\") +  \"|\" + z.getAs[String](\"income_max\"))\n      .map(e \u003d\u003e (\n            Map(ID -\u003e e._1.split(\"\\\\|\")(0)),\n            Map(household -\u003e  household_no_income_range(e._1.split(\"\\\\|\")(1),\n                    e._2.map(a \u003d\u003e household_retention_history(\n            date_range(a.getAs[Long](\"start_date_epoch\"), a.getAs[Long](\"end_date_epoch\")),\n            date_range_alt(a.getAs[String](\"start_date\"), a.getAs[String](\"end_date\")),\n            a.getAs[Boolean](\"isRetained\"))\n          )\n          )\n          ).toArray \n        ))\n\n    val updates_full \u003d householdRetentionFrame.filter(\"income_min is not null\")\n      .rdd\n      .groupBy( z \u003d\u003e z.getAs[String](\"person_key\") + \"|\" + z.getAs[String](\"household\") + \"|\" + z.getAs[String](\"income_min\") +  \"|\" + z.getAs[String](\"income_max\"))\n      .map(e \u003d\u003e (\n            Map(ID -\u003e e._1.split(\"\\\\|\")(0)),\n            Map(household -\u003e  household(e._1.split(\"\\\\|\")(1),household_income_range(e._1.split(\"\\\\|\")(2).toLong,e._1.split(\"\\\\|\")(3).toLong),\n                    e._2.map(a \u003d\u003e household_retention_history(\n            date_range(a.getAs[Long](\"start_date_epoch\"), a.getAs[Long](\"end_date_epoch\")),\n            date_range_alt(a.getAs[String](\"start_date\"), a.getAs[String](\"end_date\")),\n            a.getAs[Boolean](\"isRetained\"))\n          )\n          )\n          ).toArray \n        ))\n\n// write elasticsearch data back to the index\nupdates_full.saveToEsWithMeta(s\"${esIndexName}/person\", Map(\"es.batch.size.bytes\" -\u003e \"20mb\"))\n",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 10:54:54 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\ndefined class date_range\n\ndefined class date_range_alt\n\ndefined class household_income_range\n\ndefined class household_retention_history\n\ndefined class household\n\ndefined class household_no_income_range\n\nupdates_no_income_range: org.apache.spark.rdd.RDD[(scala.collection.immutable.Map[org.elasticsearch.spark.rdd.Metadata,String], Array[(household.type, household_no_income_range)])] \u003d MapPartitionsRDD[465] at map at \u003cconsole\u003e:78\n\nupdates_full: org.apache.spark.rdd.RDD[(scala.collection.immutable.Map[org.elasticsearch.spark.rdd.Metadata,String], Array[(household.type, household)])] \u003d MapPartitionsRDD[472] at map at \u003cconsole\u003e:76\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 499.0 failed 1 times, most recent failure: Lost task 3.0 in stage 499.0 (TID 9391, localhost, executor driver): org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: Cannot handle type [class household$] within type [class [Lscala.Tuple2;], instance [household] within instance [[Lscala.Tuple2;@4cf88241] using writer [org.elasticsearch.spark.serialization.ScalaValueWriter@773cee6f]\n\tat org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:63)\n\tat org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71)\n\tat org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58)\n\tat org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:159)\n\tat org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:67)\n\tat org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102)\n\tat org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n  at org.elasticsearch.spark.rdd.EsSpark$.doSaveToEs(EsSpark.scala:102)\n  at org.elasticsearch.spark.rdd.EsSpark$.saveToEsWithMeta(EsSpark.scala:85)\n  at org.elasticsearch.spark.rdd.EsSpark$.saveToEsWithMeta(EsSpark.scala:82)\n  at org.elasticsearch.spark.package$SparkPairRDDFunctions.saveToEsWithMeta(package.scala:73)\n  ... 60 elided\nCaused by: org.elasticsearch.hadoop.serialization.EsHadoopSerializationException: Cannot handle type [class household$] within type [class [Lscala.Tuple2;], instance [household] within instance [[Lscala.Tuple2;@4cf88241] using writer [org.elasticsearch.spark.serialization.ScalaValueWriter@773cee6f]\n  at org.elasticsearch.hadoop.serialization.builder.ContentBuilder.value(ContentBuilder.java:63)\n  at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.doWriteObject(TemplatedBulk.java:71)\n  at org.elasticsearch.hadoop.serialization.bulk.TemplatedBulk.write(TemplatedBulk.java:58)\n  at org.elasticsearch.hadoop.rest.RestRepository.writeToIndex(RestRepository.java:159)\n  at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:67)\n  at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102)\n  at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n  ... 3 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497995450083_-1519350137",
      "id": "20170620-215050_2089842839",
      "dateCreated": "Jun 20, 2017 9:50:50 PM",
      "dateStarted": "Jun 20, 2017 10:54:54 PM",
      "dateFinished": "Jun 20, 2017 10:54:58 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val joinedString \u003d \"part1|part2|2355|2233\"\nval splitString \u003d joinedString.split(\"\\\\|\")",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 10:30:21 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\njoinedString: String \u003d part1|part2|2355|2233\n\nsplitString: Array[String] \u003d Array(part1, part2, 2355, 2233)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497997673137_1090805538",
      "id": "20170620-222753_519760276",
      "dateCreated": "Jun 20, 2017 10:27:53 PM",
      "dateStarted": "Jun 20, 2017 10:30:21 PM",
      "dateFinished": "Jun 20, 2017 10:30:21 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "splitString(2).toLong",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 10:32:38 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nres41: Long \u003d 2355\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497997701619_2072715380",
      "id": "20170620-222821_243755686",
      "dateCreated": "Jun 20, 2017 10:28:21 PM",
      "dateStarted": "Jun 20, 2017 10:32:38 PM",
      "dateFinished": "Jun 20, 2017 10:32:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//updates.take(10).foreach(println) // { println  }\n// println(s\"${updates_encounter.take(10).mkString(\",\")}\") \n// updates.take(10).foreach(println) // { println  }\n updates_no_income_range.collect\n//updates.toDF().show()\n//println(s\"${updates_encounter.take(10).mkString(\",\")}\") \n// println(s\"${updates.take(10).mkString(\",\")}\") \n",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 10:53:44 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res69: Array[(scala.collection.immutable.Map[org.elasticsearch.spark.rdd.Metadata,String], Array[(household.type, household_no_income_range)])] \u003d Array((Map(ID -\u003e DEMO-6a8a34db-179b-4e9e-9d3b-3075042a9d1b),Array((household,household_no_income_range(994112315,List(household_retention_history(date_range(1370044800000,1498867199000),date_range_alt(2013-06-01,2017-06-30 23:59:59),false)))))), (Map(ID -\u003e DEMO-0c2e3778-7af2-48c6-afe2-7dc198c0167b),Array((household,household_no_income_range(994114100,List(household_retention_history(date_range(1370044800000,1401580799000),date_range_alt(2013-06-01,2014-05-31 23:59:59),false), household_retention_history(date_range(1404172800000,1498867199000),date_range_alt(2014-07-01,2017-06-30 23:59:59),false), household_retention_history(date_range(14015808..."
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1497995493217_-372151510",
      "id": "20170620-215133_1656164058",
      "dateCreated": "Jun 20, 2017 9:51:33 PM",
      "dateStarted": "Jun 20, 2017 10:53:44 PM",
      "dateFinished": "Jun 20, 2017 10:53:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "updates.saveToEsWithMeta(s\"${esIndexName}/person\", Map(\"es.batch.size.bytes\" -\u003e \"20mb\"))\n",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 9:54:05 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1497995626887_-1238389850",
      "id": "20170620-215346_1686605452",
      "dateCreated": "Jun 20, 2017 9:53:46 PM",
      "dateStarted": "Jun 20, 2017 9:54:05 PM",
      "dateFinished": "Jun 20, 2017 9:55:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "Jun 20, 2017 10:34:24 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1497995418873_-809534389",
      "id": "20170620-215018_276717934",
      "dateCreated": "Jun 20, 2017 9:50:18 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/household_retention/DRAFT/1.0 - Household Retention Processing - With Encounters",
  "id": "2CJWS8UTF",
  "angularObjects": {
    "2CK39QPEV:shared_process": [],
    "2CHGDFEAS:shared_process": [],
    "2CJPWUK64:shared_process": [],
    "2CJPRUUCQ:shared_process": [],
    "2CMA6NN69:shared_process": [],
    "2CJM2HZ3E:shared_process": [],
    "2CHUKUNGV:shared_process": [],
    "2CMSNT58Q:shared_process": [],
    "2CMY621JM:shared_process": []
  },
  "config": {},
  "info": {}
}