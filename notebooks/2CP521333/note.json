{
  "paragraphs": [
    {
      "text": "// dependencies required for elasticsearch direct access via REST client interface\nimport org.apache.http.client.methods.HttpPost\nimport org.apache.http.entity.StringEntity\nimport org.apache.http.impl.client.DefaultHttpClient\nimport org.apache.http.impl.client.BasicResponseHandler\nimport java.util.Date\nimport java.util.Calendar\nimport org.apache.commons.lang3.time.DateUtils\nimport java.text.SimpleDateFormat\nimport scala.collection.mutable.WrappedArray\n\n// dependencies\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.SQLContext._\nimport org.apache.spark.sql.functions._\n\nimport org.elasticsearch.spark.sql._\nimport org.elasticsearch.spark._\nimport org.elasticsearch.spark.rdd.Metadata._ \n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.sql.SparkSession\n\nimport spark.implicits._\n",
      "dateUpdated": "Jun 22, 2017 2:48:48 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nimport org.apache.http.client.methods.HttpPost\n\nimport org.apache.http.entity.StringEntity\n\nimport org.apache.http.impl.client.DefaultHttpClient\n\nimport org.apache.http.impl.client.BasicResponseHandler\n\nimport java.util.Date\n\nimport java.util.Calendar\n\nimport org.apache.commons.lang3.time.DateUtils\n\nimport java.text.SimpleDateFormat\n\nimport scala.collection.mutable.WrappedArray\n\nimport org.apache.spark.sql.SQLContext\n\nimport org.apache.spark.sql.SQLContext._\n\nimport org.apache.spark.sql.functions._\n\nimport org.elasticsearch.spark.sql._\n\nimport org.elasticsearch.spark._\n\nimport org.elasticsearch.spark.rdd.Metadata._\n\nimport org.apache.spark.SparkContext\n\nimport org.apache.spark.SparkContext._\n\nimport org.apache.spark.sql.SparkSession\n\nimport spark.implicits._\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498142812239_1645039831",
      "id": "20170620-145835_1359905521",
      "dateCreated": "Jun 22, 2017 2:46:52 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val clientCode \u003d \"DEMO\"\nval esIndexName \u003d \"exp_rjj_1_3\"\nval esPort \u003d 9200\nval esHost \u003d \"elasticsearch.exp-dev.io\"\n",
      "dateUpdated": "Jun 22, 2017 2:46:52 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nclientCode: String \u003d DEMO\n\nesIndexName: String \u003d exp_rjj_1_3\n\nesPort: Int \u003d 9200\n\nesHost: String \u003d elasticsearch.exp-dev.io\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498142812240_1655428051",
      "id": "20170620-145933_538760620",
      "dateCreated": "Jun 22, 2017 2:46:52 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def getHouseholdsWithoutRetention(spark:SparkSession,clientCode:String,esIndexName:String,esHost:String,esPort:Int,resultLimit:Int \u003d 10000) :(String,Int) \u003d {\n\n\n    val sc \u003d spark.sparkContext\n    val sqlContext \u003d spark.sqlContext\n    import sqlContext.implicits._    \n\n    val householdsWithoutRetention \u003d s\"\"\"\n    {\n      \"size\": 0,\n      \"_source\": [\"household.household_number\"],\n      \"query\": {\n        \"constant_score\": {\n          \"filter\": {\n            \"bool\": {\n              \"must\": [\n                {\"term\":{\"client_code\" : \"${clientCode}\" }},\n                {\n                  \"has_child\": {\n                    \"child_type\": \"encounter\",\n                    \"query\": {\n                      \"bool\": {\n                        \"must\": [\n                          { \"term\": {\"client_code\": \"${clientCode}\" }},\n                          { \"range\": {\"admit_date\": {\"gte\": \"now-4y\", \"lte\": \"now\"}}}\n                        ]\n                      }\n                    }\n                  }\n                },\n                {\"exists\": { \"field\": \"household.household_number\"} }\n              ],\n              \"must_not\": [\n                {\n                  \"nested\": {\n                    \"path\": \"household.household_retention_history\",\n                    \"query\": {\n                      \"exists\": { \"field\": \"household.household_retention_history.retained\"}\n                    }\n                  }\n                }\n              ]\n            }\n          }\n        }\n      },\n      \"aggs\": {\n        \"all_households\": {\n          \"terms\": {\n            \"field\": \"household.household_number\",\n            \"size\": ${resultLimit},\n            \"order\": {\"_term\":\"asc\"}\n          }\n        }\n      }\n    }\n    \"\"\"\n\n    // generate the elasticsearch request string\n    val endPoint \u003d \"/\"+ esIndexName + \"/person/_search\"\n    val request_url \u003d \"http://\" + esHost + \":\" + esPort + endPoint\n    \n    // build the apache HTTP post request\n    val post \u003d new HttpPost(request_url)\n    // set header for json request string\n    val setHeaderReturn \u003d post.setHeader(\"Content-type\",\"application/json\")\n    // ad the json request string to the http post\n    val setEntityReturn \u003d post.setEntity(new StringEntity(householdsWithoutRetention))\n    // send the request to elasticsearch\n    val response \u003d (new DefaultHttpClient).execute(post)\n    // get the status -- this code doesn\u0027t check for HTTP/1.1 200 OK response but the final code should!\n    val status \u003d response.getStatusLine\n    // get the json response body\n    val responseBody \u003d (new BasicResponseHandler).handleResponse(response).trim.toString\n    \n    // check the results to ensure there were any households returned\n    val dfCount \u003d sqlContext.read.json(sc.parallelize(responseBody::Nil)).select($\"hits.total\")\n    \n    val householdTotalHits \u003d dfCount.first().getLong(0).asInstanceOf[Int]\n    \n    if (householdTotalHits \u003d\u003d 0) {\n        return (null,householdTotalHits)\n    }\n    \n    val df \u003d sqlContext.read.json(sc.parallelize(responseBody::Nil)).select($\"aggregations.all_households.buckets.key\")\n    \n    val idList \u003d df.first().getAs[WrappedArray[String]](0)\n    \n    val idlist_length \u003d idList.length\n    println(s\"idlist length ${idlist_length}\")\n    \n    if (idlist_length \u003d\u003d 0) {\n        return (null,idlist_length)\n    }\n    \n    val idString \u003d idList.mkString(\"\\\"\",\"\\\",\\n\\\"\",\"\\\"\")\n    \n    println (s\"returned ${resultLimit} results\")\n\n    return (idString, idlist_length)\n}",
      "dateUpdated": "Jun 22, 2017 2:48:48 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\ngetHouseholdsWithoutRetention: (spark: org.apache.spark.sql.SparkSession, clientCode: String, esIndexName: String, esHost: String, esPort: Int, resultLimit: Int)(String, Int)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498142812240_1655428051",
      "id": "20170622-133855_1912706075",
      "dateCreated": "Jun 22, 2017 2:46:52 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def getPersonSourceData(spark:SparkSession,clientCode:String,esIndexName:String,esServer:String) :org.apache.spark.sql.DataFrame \u003d {\n\n    val sc \u003d spark.sparkContext\n    val sqlContext \u003d spark.sqlContext\n    import sqlContext.implicits._    \n\n    val personSourceQuery \u003d s\"\"\"\n{\n  \"_source\": [\"_recordId\",\"household\"],\n  \"query\": {\n    \"constant_score\": {\n      \"filter\": {\n        \"bool\": {\n          \"must\": [\n            {\"term\":{\"client_code\" : \"${clientCode}\" }},\n            {\"exists\": { \"field\": \"household.household_number\"} }\n          ],\n          \"must_not\": [\n            {\n              \"nested\": {\n                \"path\": \"household.household_retention_history\",\n                \"query\": {\n                  \"exists\": { \"field\": \"household.household_retention_history.retained\"}\n                }\n              }\n            }\n          ]\n        }\n      }\n    }\n  }\n}\n    \"\"\"\n    // limit the fields that are included\n    val personSourceQueryOptions \u003d Map(\n    //  \"es.read.field.include\" -\u003e \"_recordId,household\"\n      \"es.read.field.exclude\" -\u003e \"person_key,address,birth_date,children_present,client_code,client_name,communication,email,ethnicity,first_name,gender,language,last_name,marital_status,middle_initial,middle_name,mobile_phone,payor_category,payor_category_confidence,prefix,race,religion,suffix,campaigns,recency_frequency,perceptual_profile,chui,pdi,patient_lifecycle_history,deceased_date\",\n        \"es.read.metadata\" -\u003e \"true\", \"es.nodes\" -\u003e esServer, \"es.write.operation\" -\u003e \"upsert\", \"es.nodes.wan.only\" -\u003e \"true\",\"es.index.auto.create\" -\u003e \"false\"\n      )\n    \n    val personsES \u003d sqlContext.esDF(s\"${esIndexName}/person\", personSourceQuery, personSourceQueryOptions)\n    personsES.cache\n    personsES.createOrReplaceTempView(\"personsES\")\n    return personsES\n}\n",
      "dateUpdated": "Jun 22, 2017 3:25:00 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\ngetPersonSourceData: (spark: org.apache.spark.sql.SparkSession, clientCode: String, esIndexName: String, esServer: String)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498142812240_1655428051",
      "id": "20170620-152814_1878905727",
      "dateCreated": "Jun 22, 2017 2:46:52 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\n// return the epoch values that represent the upper and lower bounds for our not retained ranges\ndef getRetentionMonthBounds() :(Long, Long) \u003d {\n    val current_month \u003d DateUtils.truncate(new Date(),Calendar.MONTH)\n    val min_month \u003d DateUtils.addYears(current_month,-4)\n    val max_month \u003d DateUtils.addYears(current_month,10)\n    \n    val min_month_epoch \u003d min_month.getTime()\n    val max_month_epoch \u003d max_month.getTime()\n    return (min_month_epoch,max_month_epoch)\n}\n",
      "dateUpdated": "Jun 22, 2017 2:46:52 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\ngetRetentionMonthBounds: ()(Long, Long)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498142812240_1655428051",
      "id": "20170620-154345_1069029982",
      "dateCreated": "Jun 22, 2017 2:46:52 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def buildHouseholdRetentionFinal(spark:SparkSession,min_month_epoch:Long,max_month_epoch:Long,personView:String \u003d \"personsES\") :org.apache.spark.sql.DataFrame \u003d {\n\n    val sc \u003d spark.sparkContext\n    val sqlContext \u003d spark.sqlContext\n    import sqlContext.implicits._    \n\n    // BUILD THE FINAL FRAME THAT WILL BE USED TO WRITE NON-RETAINED DATA BACK TO ELASTICSEARCH\n    val householdRetentionFinal\u003d spark.sql(s\"\"\"\n        select\n        _metadata._id as person_key,\n        cast(household.household_income_range.minimum as Long) as income_min,\n        cast(household.household_income_range.maximum as Long) as income_max,\n        household.household_number as household,\n        date_format(from_unixtime(${min_month_epoch} / 1000),\u0027yyyy-MM-dd\u0027) as start_date,\n        date_format(from_unixtime(${max_month_epoch} / 1000),\u0027yyyy-MM-dd HH:mm:ss\u0027) as end_date,\n        ${min_month_epoch} as start_date_epoch,\n        ${max_month_epoch} as end_date_epoch,\n        false as isRetained\n        from ${personView}\n    \"\"\")\n    householdRetentionFinal.cache\n    householdRetentionFinal.createOrReplaceTempView(\"householdRetentionFinal\")\n\n    return householdRetentionFinal\n}",
      "dateUpdated": "Jun 22, 2017 3:25:00 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbuildHouseholdRetentionFinal: (spark: org.apache.spark.sql.SparkSession, min_month_epoch: Long, max_month_epoch: Long, personView: String)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498142812241_1655043302",
      "id": "20170620-184231_143535807",
      "dateCreated": "Jun 22, 2017 2:46:52 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "case class date_range (\n    gte: Long,\n    lte: Long\n    )\ncase class date_range_alt (\n    gte: String,\n    lte: String\n    )\ncase class household_income_range (\n    minimum: Long,\n    maximum: Long\n    )\ncase class household_retention_history (\n    date_range: date_range,\n    date_range_alt: date_range_alt,\n    retained: Boolean\n    )\ncase class household (\n    household_number: String,\n    household_income_range: household_income_range,\n    household_retention_history: Array[household_retention_history]\n    )\n\ncase class household_no_income_range (\n    household_number: String,\n    household_retention_history: Array[household_retention_history]\n    )\n\n// build combined patient lifecycle data set\ndef writeHouseholdRetentionDataToPerson(spark:SparkSession,householdRetentionFinal:org.apache.spark.sql.DataFrame,esIndexName:String) \u003d {\n\n\n    val sc \u003d spark.sparkContext\n    val sqlContext \u003d spark.sqlContext\n    import sqlContext.implicits._    \n    \n    val updates_full \u003d householdRetentionFinal.filter(\"income_min is not null and income_max is not null\")\n      .rdd\n      .groupBy( z \u003d\u003e z.getAs[String](\"person_key\") + \"|\" + z.getAs[String](\"household\") + \"|\" + z.getAs[String](\"income_min\") +  \"|\" + z.getAs[String](\"income_max\"))\n      .map(e \u003d\u003e (\n            Map(ID -\u003e e._1.split(\"\\\\|\")(0)),\n            Map(household -\u003e  household(e._1.split(\"\\\\|\")(1),household_income_range(e._1.split(\"\\\\|\")(2).toLong,e._1.split(\"\\\\|\")(3).toLong),\n                    e._2.map(a \u003d\u003e household_retention_history(\n            date_range(a.getAs[Long](\"start_date_epoch\"), a.getAs[Long](\"end_date_epoch\")),\n            date_range_alt(a.getAs[String](\"start_date\"), a.getAs[String](\"end_date\")),\n            a.getAs[Boolean](\"isRetained\"))\n          ).toArray \n          )\n          )\n        ))\n\n\n    // write elasticsearch data back to the index\n    updates_full.saveToEsWithMeta(s\"${esIndexName}/person\", Map(\"es.batch.size.bytes\" -\u003e \"20mb\"))\n\n\n    val updates_no_income_range \u003d householdRetentionFinal.filter(\"income_min is null or income_max is null\")\n      .rdd\n      .groupBy( z \u003d\u003e z.getAs[String](\"person_key\") + \"|\" + z.getAs[String](\"household\") + \"|\" + z.getAs[String](\"income_min\") +  \"|\" + z.getAs[String](\"income_max\"))\n      .map(e \u003d\u003e (\n            Map(ID -\u003e e._1.split(\"\\\\|\")(0)),\n            Map(household -\u003e  household_no_income_range(e._1.split(\"\\\\|\")(1),\n                    e._2.map(a \u003d\u003e household_retention_history(\n            date_range(a.getAs[Long](\"start_date_epoch\"), a.getAs[Long](\"end_date_epoch\")),\n            date_range_alt(a.getAs[String](\"start_date\"), a.getAs[String](\"end_date\")),\n            a.getAs[Boolean](\"isRetained\"))\n          ).toArray \n          )\n          )\n        ))\n\n\n    // write elasticsearch data back to the index\n    updates_no_income_range.saveToEsWithMeta(s\"${esIndexName}/person\", Map(\"es.batch.size.bytes\" -\u003e \"20mb\"))\n\n}\n",
      "dateUpdated": "Jun 22, 2017 2:46:52 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "results": {},
        "enabled": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\ndefined class date_range\n\ndefined class date_range_alt\n\ndefined class household_income_range\n\ndefined class household_retention_history\n\ndefined class household\n\ndefined class household_no_income_range\n\nwriteHouseholdRetentionDataToPerson: (spark: org.apache.spark.sql.SparkSession, householdRetentionFinal: org.apache.spark.sql.DataFrame, esIndexName: String)Unit\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498142812241_1655043302",
      "id": "20170620-213925_459246789",
      "dateCreated": "Jun 22, 2017 2:46:52 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "case class householdRetentionNotReadyException(private val message: String \u003d \"\", private val cause: Throwable \u003d None.orNull) extends Exception(message, cause) \n\nval (householdList,householdCount) \u003d getHouseholdsWithoutRetention(spark,clientCode,esIndexName,esHost,esPort)\nif (householdCount \u003e 0) {\n    throw new householdRetentionNotReadyException(\"households eligible for retention are available and not processed.  Household retention processing step 1 needs to be run to completion before running step 2.\")\n}\nval (min_month_epoch,max_month_epoch) \u003d getRetentionMonthBounds()\nval personsES \u003d getPersonSourceData(spark,clientCode,esIndexName,esHost)\nval householdRetentionFinal \u003d buildHouseholdRetentionFinal(spark,min_month_epoch,max_month_epoch)\nwriteHouseholdRetentionDataToPerson(spark,householdRetentionFinal,esIndexName)\n",
      "dateUpdated": "Jun 22, 2017 2:46:52 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\n\nhouseholdList: String \u003d null\nhouseholdCount: Int \u003d 0\n\n\nmin_month_epoch: Long \u003d 1370044800000\nmax_month_epoch: Long \u003d 1811808000000\n\npersonsES: org.apache.spark.sql.DataFrame \u003d [_recordId: string, household: struct\u003cemail_address: string, household_income_range: struct\u003cmaximum: float, minimum: float\u003e ... 2 more fields\u003e ... 1 more field]\n\nhouseholdRetentionFinal: org.apache.spark.sql.DataFrame \u003d [person_key: string, income_min: bigint ... 7 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498142812241_1655043302",
      "id": "20170622-142437_1812874342",
      "dateCreated": "Jun 22, 2017 2:46:52 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val a \u003d new Array[String](2)\n",
      "user": "anonymous",
      "dateUpdated": "Jun 22, 2017 6:57:42 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\na: Array[String] \u003d Array(null, null)\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498142812242_1656197549",
      "id": "20170622-144351_742227643",
      "dateCreated": "Jun 22, 2017 2:46:52 PM",
      "dateStarted": "Jun 22, 2017 6:57:42 PM",
      "dateFinished": "Jun 22, 2017 6:57:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "a(0)  \u003d \"foob\"\na(1) \u003d \"bar\"\na.length",
      "user": "anonymous",
      "dateUpdated": "Jun 22, 2017 6:58:16 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nres20: Int \u003d 2\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1498157850947_2004966855",
      "id": "20170622-185730_1255299829",
      "dateCreated": "Jun 22, 2017 6:57:30 PM",
      "dateStarted": "Jun 22, 2017 6:58:16 PM",
      "dateFinished": "Jun 22, 2017 6:58:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": " ",
      "user": "anonymous",
      "dateUpdated": "Jun 22, 2017 10:05:37 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1498157873504_-1947558596",
      "id": "20170622-185753_926196275",
      "dateCreated": "Jun 22, 2017 6:57:53 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/household_retention/FINAL/0.2 - Household Retention Processing - Inverse Not Retained",
  "id": "2CP521333",
  "angularObjects": {
    "2CK39QPEV:shared_process": [],
    "2CHGDFEAS:shared_process": [],
    "2CJPWUK64:shared_process": [],
    "2CJPRUUCQ:shared_process": [],
    "2CMA6NN69:shared_process": [],
    "2CJM2HZ3E:shared_process": [],
    "2CHUKUNGV:shared_process": [],
    "2CMSNT58Q:shared_process": [],
    "2CMY621JM:shared_process": []
  },
  "config": {},
  "info": {}
}