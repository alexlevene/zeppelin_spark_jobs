{
  "paragraphs": [
    {
      "title": "Add ES Spark dependency",
      "text": "%spark.dep\n// Load our Elasticsearch spark dependencies from Maven\nz.reset()\nz.load(\"org.elasticsearch:elasticsearch-spark-20_2.11:5.4.1\")\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:24:57 PM",
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res0: org.apache.zeppelin.dep.Dependency \u003d org.apache.zeppelin.dep.Dependency@3dcfecd3\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502854014163_2097702254",
      "id": "20170811-190646_251057904",
      "dateCreated": "Aug 16, 2017 3:26:54 AM",
      "dateStarted": "Aug 22, 2017 11:24:58 PM",
      "dateFinished": "Aug 22, 2017 11:25:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Import dependencies",
      "text": "// Import our dependencies\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.SQLContext    \nimport org.apache.spark.sql.SQLContext._\nimport org.elasticsearch.spark.sql._\nimport org.elasticsearch.spark._\nimport org.elasticsearch.spark.rdd.EsSpark\nimport org.elasticsearch.spark.rdd.Metadata._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql\nimport org.apache.spark.sql.types._\nimport org.apache.http.client.methods.HttpPost\nimport org.apache.http.entity.StringEntity\nimport org.apache.http.impl.client.DefaultHttpClient\nimport org.apache.http.impl.client.BasicResponseHandler\nimport java.util.Date\nimport java.util.Calendar\nimport org.apache.commons.lang3.time.DateUtils\nimport java.text.SimpleDateFormat\nimport scala.collection.mutable.WrappedArray\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:24:58 PM",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nimport org.apache.spark.sql.SparkSession\n\nimport org.apache.spark.sql.DataFrame\n\nimport org.apache.spark.sql.SQLContext\n\nimport org.apache.spark.sql.SQLContext._\n\nimport org.elasticsearch.spark.sql._\n\nimport org.elasticsearch.spark._\n\nimport org.elasticsearch.spark.rdd.EsSpark\n\nimport org.elasticsearch.spark.rdd.Metadata._\n\nimport org.apache.spark.SparkContext\n\nimport org.apache.spark.SparkContext._\n\nimport org.apache.spark.SparkConf\n\nimport org.apache.spark.sql\n\nimport org.apache.spark.sql.types._\n\nimport org.apache.http.client.methods.HttpPost\n\nimport org.apache.http.entity.StringEntity\n\nimport org.apache.http.impl.client.DefaultHttpClient\n\nimport org.apache.http.impl.client.BasicResponseHandler\n\nimport java.util.Date\n\nimport java.util.Calendar\n\nimport org.apache.commons.lang3.time.DateUtils\n\nimport java.text.SimpleDateFormat\n\nimport scala.collection.mutable.WrappedArray\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502854014195_2085390289",
      "id": "20170811-190656_554437167",
      "dateCreated": "Aug 16, 2017 3:26:54 AM",
      "dateStarted": "Aug 22, 2017 11:25:03 PM",
      "dateFinished": "Aug 22, 2017 11:25:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Set vars",
      "text": "val esServer \u003d \"exp-elasticsearch.default.svc.cluster.local\"\nval esPort \u003d 9200\nval clientCode \u003d \"DEMO\"\nval esIndexName \u003d \"exp_v1_0_1_patlc_alex\"\nval resultLimit \u003d 100\nval batchSize \u003d resultLimit\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:24:58 PM",
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nesServer: String \u003d exp-elasticsearch.default.svc.cluster.local\n\nesPort: Int \u003d 9200\n\nclientCode: String \u003d DEMO\n\nesIndexName: String \u003d exp_v1_0_1_patlc_alex\n\nresultLimit: Int \u003d 100\n\nbatchSize: Int \u003d 100\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502854014204_2080388553",
      "id": "20170811-191652_1223362553",
      "dateCreated": "Aug 16, 2017 3:26:54 AM",
      "dateStarted": "Aug 22, 2017 11:25:17 PM",
      "dateFinished": "Aug 22, 2017 11:25:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "restCall definition",
      "text": "  def restCall(endPoint:String, esServer:String, esPort:Int, postBody:String):String \u003d {\r\n    val request_url \u003d \"https://\" + esServer + \":\" + esPort + endPoint\r\n\r\n    // build the apache HTTP post request\r\n    val post \u003d new HttpPost(request_url)\r\n    // set header for json request string\r\n    val setHeaderReturn \u003d post.setHeader(\"Content-type\",\"application/json\")\r\n    // ad the json request string to the http post\r\n    val setEntityReturn \u003d post.setEntity(new StringEntity(postBody))\r\n    // send the request to elasticsearch\r\n    val response \u003d (new DefaultHttpClient).execute(post)\r\n    // get the status -- this code doesn\u0027t check for HTTP/1.1 200 OK response but the final code should!\r\n    val status \u003d response.getStatusLine\r\n    // get the json response body\r\n    val responseBody \u003d (new BasicResponseHandler).handleResponse(response).trim.toString\r\n    \r\n    return responseBody\r\n  }\r\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:24:58 PM",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nrestCall: (endPoint: String, esServer: String, esPort: Int, postBody: String)String\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502854014209_-2130318734",
      "id": "20170816-031239_1155671196",
      "dateCreated": "Aug 16, 2017 3:26:54 AM",
      "dateStarted": "Aug 22, 2017 11:25:41 PM",
      "dateFinished": "Aug 22, 2017 11:25:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "def countPersonsWithoutEncounterLifecycle",
      "text": "    def countPersonsWithoutEncounterLifecycle(spark:SparkSession,clientCode:String,esIndexName:String,esHost:String,esPort:Int):Int \u003d {\r\n        val sc \u003d spark.sparkContext\r\n        val sqlContext \u003d spark.sqlContext\r\n        import sqlContext.implicits._    \r\n        val personsWithoutEncounterLifecycle \u003d s\"\"\"\r\n        {\r\n            \"size\": 0,\r\n            \"query\": {\r\n              \"constant_score\": {\r\n                \"filter\": {\r\n                  \"bool\": {\r\n                    \"must\": [\r\n                      {\"term\":{\"client_code\" : \"${clientCode}\" }},\r\n                      {\r\n                        \"has_child\": {\r\n                          \"type\": \"encounter\",\r\n                          \"query\": {\r\n                            \"bool\": {\r\n                              \"must\": [\r\n                                { \"term\": {\"client_code\": \"${clientCode}\" }}\r\n                              ]\r\n                            }\r\n                          }\r\n                        } \r\n                      }\r\n                    ],\r\n                    \"must_not\": [\r\n                      {\r\n                        \"has_child\": {\r\n                          \"type\": \"encounter\",\r\n                          \"query\": {\r\n                            \"nested\": {\r\n                              \"path\": \"patient_lifecycle\",\r\n                              \"query\": {\r\n                                \"exists\": { \"field\": \"patient_lifecycle.state_flags.re_engaged\"}\r\n                              }\r\n                            }\r\n                          }\r\n                        }\r\n                      }\r\n                    ]\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        \"\"\"\r\n        // generate the elasticsearch request string\r\n        val endPoint \u003d \"/\"+ esIndexName + \"/person/_search\"\r\n        val responseBody \u003d restCall(endPoint, esHost, esPort, personsWithoutEncounterLifecycle)\r\n\r\n        // check the results to ensure there were any households returned\r\n        val dfCount \u003d sqlContext.read.json(sc.parallelize(responseBody::Nil)).select($\"hits.total\")\r\n        return dfCount.first().getLong(0).asInstanceOf[Int]\r\n    }\r\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:24:59 PM",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\ncountPersonsWithoutEncounterLifecycle: (spark: org.apache.spark.sql.SparkSession, clientCode: String, esIndexName: String, esHost: String, esPort: Int)Int\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502854014215_-2131088231",
      "id": "20170815-214906_139962624",
      "dateCreated": "Aug 16, 2017 3:26:54 AM",
      "dateStarted": "Aug 22, 2017 11:25:44 PM",
      "dateFinished": "Aug 22, 2017 11:25:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "def getEncounterSourceData",
      "text": "    def getEncounterSourceData(spark:SparkSession,clientCode:String,esIndexName:String,esServer:String,esPort:Int,resultLimit:Int):org.apache.spark.sql.DataFrame \u003d {\r\n\r\n        val sc \u003d spark.sparkContext\r\n        val sqlContext \u003d spark.sqlContext\r\n        import sqlContext.implicits._\r\n\r\n\t\tval allEncountersForPersonsWithoutPatientLifecycleFlags \u003d s\"\"\"\r\n\t\t{\r\n\t\t  \"query\": {\r\n\t\t\t\"has_parent\": {\r\n\t\t\t  \"parent_type\": \"person\",\r\n\t\t\t  \"query\": {\r\n\t\t\t\t\"bool\": {\r\n\t\t\t\t  \"must\": [\r\n\t\t\t\t\t{\"term\":{\"client_code\" : \"${clientCode}\" }},\r\n\t\t\t\t\t{\r\n\t\t\t\t\t  \"has_child\": {\r\n\t\t\t\t\t\t\"type\": \"encounter\",\r\n\t\t\t\t\t\t\"query\": {\r\n\t\t\t\t\t\t  \"bool\": {\r\n\t\t\t\t\t\t\t\"must\": [\r\n\t\t\t\t\t\t\t  { \"term\": {\"client_code\": \"${clientCode}\" }}\r\n\t\t\t\t\t\t\t]\r\n\t\t\t\t\t\t  }\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t  } \r\n\t\t\t\t\t}\r\n\t\t\t\t  ],\r\n\t\t\t\t  \"must_not\": [\r\n\t\t\t\t\t{\r\n\t\t\t\t\t  \"has_child\": {\r\n\t\t\t\t\t\t\"type\": \"encounter\",\r\n\t\t\t\t\t\t\"query\": {\r\n\t\t\t\t\t\t  \"nested\": {\r\n\t\t\t\t\t\t\t\"path\": \"patient_lifecycle\",\r\n\t\t\t\t\t\t\t\"query\": {\r\n\t\t\t\t\t\t\t  \"exists\": { \"field\": \"patient_lifecycle.state_flags.re_engaged\"}\r\n\t\t\t\t\t\t\t}\r\n\t\t\t\t\t\t  }\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t  }\r\n\t\t\t\t\t}\r\n\t\t\t\t  ]\r\n\t\t\t\t}\r\n\t\t\t  }\r\n\t\t\t}\r\n\t\t  }\r\n\t\t}\r\n\t\t\"\"\"\r\n\r\n        val encounterSourceQueryOptions \u003d Map(\r\n          \"es.read.source.filter\" -\u003e \"admit_date,facility_code,service_category\",\r\n          \"es.read.metadata\" -\u003e \"true\"\r\n        )\r\n        \r\n        val allEncountersMissingFlagsRDD \u003d EsSpark.esJsonRDD(sc, s\"${esIndexName}/encounter\", allEncountersForPersonsWithoutPatientLifecycleFlags, encounterSourceQueryOptions)\r\n\t\t\r\n\t\t// get jsonMap of response as second item of tuple\r\n\t\tval encountersJson \u003d allEncountersMissingFlagsRDD.map[String](d \u003d\u003e d._2)\r\n\t\tencountersJson.cache\r\n\t\t\r\n\t\t// read json docs into dataframe\r\n\t\tval allEncountersMissingFlagsDF \u003d sqlContext.read.json(encountersJson)\r\n\t\tallEncountersMissingFlagsDF.cache\r\n\t\tval encounterCount \u003d allEncountersMissingFlagsDF.count\r\n\t\tprintln(s\"total encounter count for persons with patient lifecycle not present: ${encounterCount}\")\r\n\r\n\t\t// select and rename fields\r\n\t\tval encountersES \u003d allEncountersMissingFlagsDF.select(\r\n\t\t\t$\"_metadata._id\".as(\"encounter_id\"),\r\n\t\t\t$\"_metadata._parent\".as(\"person_id\"),\r\n\t\t\t$\"admit_date\",\r\n\t\t\t$\"facility_code\",\r\n\t\t\t$\"service_category.code\".as(\"service_category_code\"))\r\n\t\tencountersES.cache\r\n\t\tencountersES.createOrReplaceTempView(\"encountersES\")\r\n        return encountersES\r\n    }\r\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:24:59 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\ngetEncounterSourceData: (spark: org.apache.spark.sql.SparkSession, clientCode: String, esIndexName: String, esServer: String, esPort: Int, resultLimit: Int)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502903046447_637619884",
      "id": "20170816-170406_1004395965",
      "dateCreated": "Aug 16, 2017 5:04:06 PM",
      "dateStarted": "Aug 22, 2017 11:25:45 PM",
      "dateFinished": "Aug 22, 2017 11:25:49 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "def buildEncounterBaseFromSource - establish sequences across encounters (overall, service_category, facility)",
      "text": "    def buildEncounterBaseFromSource(spark:SparkSession) :org.apache.spark.sql.DataFrame \u003d {\r\n\r\n        val sc \u003d spark.sparkContext\r\n        val sqlContext \u003d spark.sqlContext\r\n        import sqlContext.implicits._    \r\n\r\n        val encounterSelect \u003d spark.sql(\"\"\"\r\n            SELECT\r\n              encounter_id,\r\n              person_id,\r\n              cast(admit_date as timestamp) as admit_date,\r\n              facility_code as facility_code,\r\n              service_category_code as service_category_code,\r\n              row_number() over (partition by person_id order by admit_date asc, encounter_id asc) as enc_seq ,\r\n              row_number() over (partition by person_id order by admit_date asc, encounter_id asc) -1 as enc_prev,\r\n              row_number() over (partition by person_id,service_category_code order by admit_date asc, encounter_id asc) as enc_seq_sc,\r\n              row_number() over (partition by person_id,service_category_code order by admit_date asc, encounter_id asc) -1 as enc_prev_sc,\r\n              row_number() over (partition by person_id,facility_code order by admit_date asc, encounter_id asc) as enc_seq_f,\r\n              row_number() over (partition by person_id,facility_code order by admit_date asc, encounter_id asc) -1 as enc_prev_f\r\n            FROM encountersES\r\n        \"\"\")\r\n        encounterSelect.createOrReplaceTempView(\"encounter\")\r\n        encounterSelect.cache()\r\n\r\n        return encounterSelect\r\n    }\r\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:24:59 PM",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbuildEncounterBaseFromSource: (spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502854014237_-2128779738",
      "id": "20170811-220440_312130954",
      "dateCreated": "Aug 16, 2017 3:26:54 AM",
      "dateStarted": "Aug 22, 2017 11:25:47 PM",
      "dateFinished": "Aug 22, 2017 11:25:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "def buildEncounterYearsBetweenFromBase - use sequenced data to determine # years between encounters. for service category and facility, determine previous encounters from differing values",
      "text": "    def buildEncounterYearsBetweenFromBase(spark:SparkSession) :org.apache.spark.sql.DataFrame \u003d {\r\n\r\n        val sc \u003d spark.sparkContext\r\n        val sqlContext \u003d spark.sqlContext\r\n        import sqlContext.implicits._\r\n\r\n        val encounterYearsBetween \u003d spark.sql(\"\"\"\r\n          select cur.encounter_id,\r\n          cur.person_id,\r\n          cur.enc_seq,\r\n          cur.enc_prev,\r\n          case when prv.person_id is null then 99 else datediff(cur.admit_date,prv.admit_date)/365.25 end as yearsBetween,\r\n          case when prv_f.person_id is null then 99 else datediff(cur.admit_date,prv_f.admit_date)/365.25 end as yearsBetween_f,\r\n          case when prv_sc.person_id is null then 99 else datediff(cur.admit_date,prv_sc.admit_date)/365.25 end as yearsBetween_sc,\r\n          prv_not_sc.yearsbetween as yearsBetween_not_sc,\r\n          prv_not_f.yearsbetween as yearsBetween_not_f,\r\n          (case when\r\n              (case when prv_sc.person_id is null then 98 else datediff(cur.admit_date,prv_sc.admit_date)/365.25 end) -- yearsBetween_sc\r\n                  \u003c\r\n              (case when prv_f.person_id is null then 99 else datediff(cur.admit_date,prv_f.admit_date)/365.25 end) -- yearsBetween_f\r\n              then\r\n                  (case when prv_sc.person_id is null then 99 else datediff(cur.admit_date,prv_sc.admit_date)/365.25 end) -- yearsBetween_sc\r\n              else\r\n                  (case when prv_f.person_id is null then 99 else datediff(cur.admit_date,prv_f.admit_date)/365.25 end) -- yearsBetween_f\r\n              end) as yearsBetween_f_or_sc\r\n          from encounter cur\r\n          left join encounter prv on cur.person_id \u003d prv.person_id and cur.enc_prev \u003d prv.enc_seq\r\n          left join encounter prv_f on cur.person_id \u003d prv_f.person_id and cur.enc_prev_f \u003d prv_f.enc_seq_f and cur.facility_code \u003d prv_f.facility_code\r\n          left join encounter prv_sc on cur.person_id \u003d prv_sc.person_id and cur.enc_prev_sc \u003d prv_sc.enc_seq_sc and cur.service_category_code \u003d prv_sc.service_category_code\r\n          left join (\r\n              select encounter_id,\r\n              case when person_id is null then 98 else yearsbetween end as yearsbetween\r\n              from (\r\n                  select cur.encounter_id,\r\n                  prv.person_id,\r\n                  datediff(cur.admit_date,prv.admit_date)/365.25 as yearsbetween,\r\n                  row_number() over (partition by cur.person_id,cur.service_category_code,cur.enc_seq order by prv.enc_seq desc) as not_sc_seq\r\n                  from encounter cur left join encounter prv\r\n                  on prv.person_id \u003d cur.person_id\r\n                      and cur.service_category_code \u003c\u003e prv.service_category_code\r\n                      and cur.enc_seq \u003e prv.enc_seq\r\n              ) where not_sc_seq \u003d 1\r\n          ) prv_not_sc on cur.encounter_id \u003d prv_not_sc.encounter_id\r\n          left join (\r\n              select encounter_id,\r\n              case when person_id is null then 99 else yearsbetween end as yearsbetween\r\n              from (\r\n                  select cur.encounter_id,prv.person_id,\r\n                  datediff(cur.admit_date,prv.admit_date)/365.25 as yearsbetween,\r\n                  row_number() over (partition by cur.person_id,cur.facility_code,cur.enc_seq order by prv.enc_seq desc) as not_f_seq\r\n                  from encounter cur left join encounter prv\r\n                  on prv.person_id \u003d cur.person_id\r\n                      and cur.facility_code \u003c\u003e prv.facility_code\r\n                      and cur.enc_seq \u003e prv.enc_seq\r\n              ) where not_f_seq \u003d 1\r\n          ) prv_not_f on cur.encounter_id \u003d prv_not_f.encounter_id\r\n        \"\"\")\r\n        \r\n        encounterYearsBetween.cache\r\n        encounterYearsBetween.createOrReplaceTempView(\"encounterYearsBetween\")\r\n\r\n        return encounterYearsBetween\r\n    }\r\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:24:59 PM",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbuildEncounterYearsBetweenFromBase: (spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502854014259_-2135705218",
      "id": "20170814-203204_1134603726",
      "dateCreated": "Aug 16, 2017 3:26:54 AM",
      "dateStarted": "Aug 22, 2017 11:25:50 PM",
      "dateFinished": "Aug 22, 2017 11:25:52 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "def buildEncounterLifecycleFlagsFromYearsBetween - determine indicators at each encounter row based on encounter time diff values",
      "text": "    def buildEncounterLifecycleFlagsFromYearsBetween(spark:SparkSession) :org.apache.spark.sql.DataFrame \u003d {\r\n\r\n        val sc \u003d spark.sparkContext\r\n        val sqlContext \u003d spark.sqlContext\r\n        import sqlContext.implicits._\r\n\r\n\r\n        val encounterLifecycleFlags \u003d spark.sql(\"\"\"\r\n          select encounter_id,\r\n          person_id,\r\n          enc_seq,\r\n          enc_prev,\r\n          yearsBetween,\r\n          yearsBetween_f,\r\n          yearsBetween_sc,\r\n          yearsBetween_not_sc,\r\n          yearsBetween_not_f,\r\n          yearsBetween_f_or_sc,\r\n          -- new patient if they did not have any medical encounter in the previous 3 years\r\n          case when yearsbetween \u003e 3 then 1 else 0 end as new,\r\n          -- re-engaged patient if they did have an encounter in the previous 3 years and did not have an encounter in the last year\r\n          case when yearsbetween \u003e 1 and yearsbetween \u003c\u003d 3 then 1 else 0 end as re_engaged,\r\n          -- service cross-sell if they did NOT have an encounter in that service category in the past 3 years and DID have an encounter in a DIFFERENT service category in the past 3 years\r\n          case when\r\n              -- did NOT have an encounter in that service category in the past 3 years\r\n              yearsbetween_sc \u003e 3\r\n              -- DID have an encounter in a DIFFERENT service category in the past 3 years\r\n              and yearsbetween_not_sc \u003c\u003d 3\r\n              then 1 else 0 end as xs_sc,\r\n          -- facility_code cross-sell if they did NOT have an encounter at that facility_code in the past 3 years and DID have an encounter in a DIFFERENT facility_code in the past 3 years\r\n          case when\r\n              -- did NOT have an encounter at that facility_code in the past 3 years\r\n              yearsbetween_f \u003e 3\r\n              -- DID have an encounter at a DIFFERENT facility_code in the past 3 years\r\n              and yearsbetween_not_f \u003c\u003d 3\r\n              then 1 else 0 end as xs_f,\r\n          case when\r\n              -- service cross-sell\r\n              (yearsbetween_sc \u003e 3 and yearsbetween_not_sc \u003c\u003d 3)\r\n              or\r\n              -- facility_code cross-sell\r\n              (yearsbetween_f \u003e 3 and yearsbetween_not_f \u003c\u003d 3)\r\n              then 1 else 0 end as xs,\r\n          -- existing patient if they did have an encounter in the previous 3 years within the same facility_code or service category\r\n          case when yearsbetween_f_or_sc \u003c\u003d 3 then 1 else 0 end as existing\r\n          from encounterYearsBetween\r\n        \"\"\")\r\n\r\n        encounterLifecycleFlags.cache\r\n        encounterLifecycleFlags.createOrReplaceTempView(\"encounterLifecycleFlags\")\r\n\r\n        return encounterLifecycleFlags\r\n    }\r\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:24:59 PM",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbuildEncounterLifecycleFlagsFromYearsBetween: (spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502854014275_2140794131",
      "id": "20170814-211850_1818582324",
      "dateCreated": "Aug 16, 2017 3:26:54 AM",
      "dateStarted": "Aug 22, 2017 11:25:51 PM",
      "dateFinished": "Aug 22, 2017 11:25:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "def buildEncounterLifecycleStateFromFlags - apply hierarchical logic from flags to determine overall lifecycle values",
      "text": "    def buildEncounterLifecycleStateFromFlags(spark:SparkSession) :org.apache.spark.sql.DataFrame \u003d {\r\n\r\n        val sc \u003d spark.sparkContext\r\n        val sqlContext \u003d spark.sqlContext\r\n        import sqlContext.implicits._\r\n\r\n        val encounterLifecycleState \u003d spark.sql(\"\"\"\r\n          select\r\n          encounter_id,\r\n          cur.person_id,\r\n          cur.enc_seq,\r\n          enc_prev,\r\n          yearsBetween,\r\n          yearsBetween_f,\r\n          yearsBetween_sc,\r\n          yearsBetween_not_sc,\r\n          yearsBetween_not_f,\r\n          yearsBetween_f_or_sc,\r\n          cast(new as Boolean) as new,\r\n          cast(re_engaged as Boolean) as re_engaged,\r\n          cast(xs_sc as Boolean) as cross_sell_service_category,\r\n          cast(xs_f as Boolean) as cross_sell_facility,\r\n          cast(xs as Boolean) as cross_sell,\r\n          cast(existing as Boolean) as existing,\r\n          \u0027Patient\u0027 as lifecycleTypeHighLevel,\r\n          -- standard hierarchy: new - cross-service - cross-facility - re-engaged - existing\r\n          case\r\n              when new \u003d 1 then \u0027New Patient\u0027\r\n              when xs_sc \u003d 1 then \u0027Service Cross-Sell\u0027\r\n              when xs_f \u003d 1 then \u0027Facility Cross-Sell\u0027\r\n              when re_engaged \u003d 1 then \u0027Re-Engaged Patient\u0027\r\n              when existing \u003d 1 then \u0027Existing Patient\u0027\r\n              else \u0027ZZUNKNOWN\u0027\r\n          end as lifecycleType,\r\n          -- service-specific hierarchy: new - cross-service - cross - re-engaged - existing\r\n          case\r\n              when new \u003d 1 then \u0027New Patient\u0027\r\n              when xs_sc \u003d 1 then \u0027Service Cross-Sell\u0027\r\n              when xs \u003d 1 then \u0027Cross-Sell\u0027\r\n              when re_engaged \u003d 1 then \u0027Re-Engaged Patient\u0027\r\n              when existing \u003d 1 then \u0027Existing Patient\u0027\r\n              else \u0027ZZUNKNOWN\u0027\r\n          end as lifecycleType_Service,\r\n          -- facility-specific hierarchy: new - cross-facility - cross - re-engaged - existing\r\n          case\r\n              when new \u003d 1 then \u0027New Patient\u0027\r\n              when xs_f \u003d 1 then \u0027Facility Cross-Sell\u0027\r\n              when xs \u003d 1 then \u0027Cross-Sell\u0027\r\n              when re_engaged \u003d 1 then \u0027Re-Engaged Patient\u0027\r\n              when existing \u003d 1 then \u0027Existing Patient\u0027\r\n              else \u0027ZZUNKNOWN\u0027\r\n          end as lifecycleType_Facility,\r\n          case when prv.previousLifecycleType is not null then \u0027Patient\u0027 else \u0027Prospect\u0027 end as previousLifecycleTypeHighLevel,\r\n          coalesce(prv.previousLifecycleType,\u0027Prospect\u0027) as previousLifecycleType,\r\n          coalesce(prv.previousLifecycleType_Service,\u0027Prospect\u0027) as previousLifecycleType_Service,\r\n          coalesce(prv.previousLifecycleType_Facility,\u0027Prospect\u0027) as previousLifecycleType_Facility\r\n          from encounterLifecycleFlags cur\r\n          left join (\r\n              select person_id,enc_seq as prv_enc_seq,\r\n              -- standard hierarchy: new - cross-service - cross-facility - re-engaged - existing\r\n              case\r\n                  when new \u003d 1 then \u0027New Patient\u0027\r\n                  when xs_sc \u003d 1 then \u0027Service Cross-Sell\u0027\r\n                  when xs_f \u003d 1 then \u0027Facility Cross-Sell\u0027\r\n                  when re_engaged \u003d 1 then \u0027Re-Engaged Patient\u0027\r\n                  when existing \u003d 1 then \u0027Existing Patient\u0027\r\n                  else \u0027ZZUNKNOWN\u0027\r\n              end as previousLifecycleType,\r\n              -- service-specific hierarchy: new - cross-service - cross - re-engaged - existing\r\n              case\r\n                  when new \u003d 1 then \u0027New Patient\u0027\r\n                  when xs_sc \u003d 1 then \u0027Service Cross-Sell\u0027\r\n                  when xs \u003d 1 then \u0027Cross-Sell\u0027\r\n                  when re_engaged \u003d 1 then \u0027Re-Engaged Patient\u0027\r\n                  when existing \u003d 1 then \u0027Existing Patient\u0027\r\n                  else \u0027ZZUNKNOWN\u0027\r\n              end as previousLifecycleType_Service,\r\n              -- facility-specific hierarchy: new - cross-facility - cross - re-engaged - existing\r\n              case\r\n                  when new \u003d 1 then \u0027New Patient\u0027\r\n                  when xs_f \u003d 1 then \u0027Facility Cross-Sell\u0027\r\n                  when xs \u003d 1 then \u0027Cross-Sell\u0027\r\n                  when re_engaged \u003d 1 then \u0027Re-Engaged Patient\u0027\r\n                  when existing \u003d 1 then \u0027Existing Patient\u0027\r\n                  else \u0027ZZUNKNOWN\u0027\r\n              end as previousLifecycleType_Facility\r\n              from encounterLifecycleFlags\r\n          ) prv on prv.person_id \u003d cur.person_id and prv.prv_enc_seq \u003d cur.enc_prev\r\n        \"\"\")\r\n\r\n        encounterLifecycleState.cache\r\n      encounterLifecycleState.createOrReplaceTempView(\"encounterLifecycleState\")\r\n\r\n        return encounterLifecycleState\r\n    }\r\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:25:00 PM",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nbuildEncounterLifecycleStateFromFlags: (spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502854014284_2135792395",
      "id": "20170814-213500_111844456",
      "dateCreated": "Aug 16, 2017 3:26:54 AM",
      "dateStarted": "Aug 22, 2017 11:25:52 PM",
      "dateFinished": "Aug 22, 2017 11:25:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "class definitions",
      "text": "case class state_flags (\r\n    cross_sell: Boolean,\r\n    cross_sell_facility: Boolean,\r\n    cross_sell_service_category: Boolean,\r\n    existing: Boolean,\r\n    `new`: Boolean,\r\n    re_engaged: Boolean\r\n    )\r\ncase class previous_state (\r\n    high_level: String,\r\n    specific: String\r\n    )\r\ncase class current_state (\r\n    high_level: String,\r\n    specific: String\r\n    )\r\ncase class previous_state_facility (\r\n    high_level: String,\r\n    specific: String\r\n    )\r\ncase class current_state_facility (\r\n    high_level: String,\r\n    specific: String\r\n    )\r\n\r\ncase class previous_state_service (\r\n    high_level: String,\r\n    specific: String\r\n    )\r\ncase class current_state_service (\r\n    high_level: String,\r\n    specific: String\r\n    )\r\ncase class patient_lifecycle (\r\n    current_state: current_state,\r\n    previous_state: previous_state,\r\n    current_state_facility: current_state_facility,\r\n    previous_state_facility: previous_state_facility,\r\n    current_state_service: current_state_service,\r\n    previous_state_service: previous_state_service,\r\n    state_flags: state_flags\r\n    )\r\n    \r\n//    case class state_flags (\r\n//        cross_sell_service_category: Boolean,\r\n//        `new`: Boolean,\r\n//        re_engaged: Boolean\r\n//        )\r\n//    case class patient_lifecycle (\r\n//        state_flags: state_flags\r\n//        )\r\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:25:00 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\ndefined class state_flags\n\ndefined class previous_state\n\ndefined class current_state\n\ndefined class previous_state_facility\n\ndefined class current_state_facility\n\ndefined class previous_state_service\n\ndefined class current_state_service\n\ndefined class patient_lifecycle\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502854725419_510257296",
      "id": "20170816-033845_637584227",
      "dateCreated": "Aug 16, 2017 3:38:45 AM",
      "dateStarted": "Aug 22, 2017 11:25:53 PM",
      "dateFinished": "Aug 22, 2017 11:26:01 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "def writePatientLifecycleDataToEncounter",
      "text": "    def writePatientLifecycleDataToEncounter(spark:SparkSession,encounterLifecycleFrame:org.apache.spark.sql.DataFrame,esIndexName:String,esServer:String) \u003d {\r\n      val sc \u003d spark.sparkContext\r\n      val sqlContext \u003d spark.sqlContext\r\n      import sqlContext.implicits._\r\n\r\n      val updates \u003d encounterLifecycleFrame\r\n        .rdd\r\n        .map(e \u003d\u003e (\r\n          Map(\r\n            ID -\u003e e.getAs[String](\"encounter_id\"),\r\n            PARENT -\u003e e.getAs[String](\"person_id\"),\r\n            ROUTING -\u003e e.getAs[String](\"person_id\")\r\n          ),\r\n          Map(\"patient_lifecycle\" -\u003e\r\n            patient_lifecycle(\r\n                current_state(\r\n                    e.getAs[String](\"lifecycleTypeHighLevel\"),\r\n                    e.getAs[String](\"lifecycleType\")),\r\n                previous_state(\r\n                    e.getAs[String](\"previousLifecycleTypeHighLevel\"),\r\n                    e.getAs[String](\"previousLifecycleType\")),\r\n                current_state_facility(\r\n                    e.getAs[String](\"lifecycleTypeHighLevel\"),\r\n                    e.getAs[String](\"lifecycleType_Facility\")),\r\n                previous_state_facility(\r\n                    e.getAs[String](\"previousLifecycleTypeHighLevel\"),\r\n                    e.getAs[String](\"previousLifecycleType_Facility\")),\r\n                current_state_service(\r\n                    e.getAs[String](\"lifecycleTypeHighLevel\"),\r\n                    e.getAs[String](\"lifecycleType_Service\")),\r\n                previous_state_service(\r\n                    e.getAs[String](\"previousLifecycleTypeHighLevel\"),\r\n                    e.getAs[String](\"previousLifecycleType_Service\")),\r\n                state_flags(\r\n                    e.getAs[Boolean](\"cross_sell\"),\r\n                    e.getAs[Boolean](\"cross_sell_facility\"),\r\n                    e.getAs[Boolean](\"cross_sell_service_category\"),\r\n                    e.getAs[Boolean](\"existing\"),\r\n                    e.getAs[Boolean](\"new\"),\r\n                    e.getAs[Boolean](\"re_engaged\"))\r\n            )\r\n          )\r\n        )\r\n      )\r\n\r\n      var esconf \u003d Map(\r\n        \"es.nodes\" -\u003e esServer,\r\n        \"es.write.operation\" -\u003e \"upsert\",\r\n        \"es.nodes.wan.only\" -\u003e \"true\",\r\n        \"es.index.auto.create\" -\u003e \"false\",\r\n        \"es.batch.size.bytes\" -\u003e \"20mb\")\r\n\r\n      // write elasticsearch data back to the index\r\n      updates.saveToEsWithMeta(s\"${esIndexName}/encounter\", esconf)\r\n    }\r\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:26:21 PM",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "title": true,
        "results": {},
        "enabled": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nwritePatientLifecycleDataToEncounter: (spark: org.apache.spark.sql.SparkSession, encounterLifecycleFrame: org.apache.spark.sql.DataFrame, esIndexName: String, esServer: String)Unit\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502854014319_2123865179",
      "id": "20170814-214122_623175280",
      "dateCreated": "Aug 16, 2017 3:26:54 AM",
      "dateStarted": "Aug 22, 2017 11:27:44 PM",
      "dateFinished": "Aug 22, 2017 11:27:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val x  \u003d countPersonsWithoutEncounterLifecycle(spark,clientCode,esIndexName,esServer,esPort)\r\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:25:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nx: Int \u003d 7344\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502854815145_56330738",
      "id": "20170816-034015_1264407346",
      "dateCreated": "Aug 16, 2017 3:40:15 AM",
      "dateStarted": "Aug 22, 2017 11:26:02 PM",
      "dateFinished": "Aug 22, 2017 11:26:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val encounterSourceFrame \u003d getEncounterSourceData(spark,clientCode,esIndexName,esServer,esPort,batchSize)",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:25:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "total encounter count for persons with patient lifecycle not present: 26245\n\nencounterSourceFrame: org.apache.spark.sql.DataFrame \u003d [encounter_id: string, person_id: string ... 3 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1503437876127_81042276",
      "id": "20170822-213756_1180868602",
      "dateCreated": "Aug 22, 2017 9:37:56 PM",
      "dateStarted": "Aug 22, 2017 11:26:03 PM",
      "dateFinished": "Aug 22, 2017 11:27:31 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val encounterBaseFrame \u003d buildEncounterBaseFromSource(spark)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:25:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nencounterBaseFrame: org.apache.spark.sql.DataFrame \u003d [encounter_id: string, person_id: string ... 9 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1502909661212_-1918212938",
      "id": "20170816-185421_831015135",
      "dateCreated": "Aug 16, 2017 6:54:21 PM",
      "dateStarted": "Aug 22, 2017 11:26:12 PM",
      "dateFinished": "Aug 22, 2017 11:27:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "        val encounterYearsBetweenFrame \u003d buildEncounterYearsBetweenFromBase(spark)\n\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:25:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nencounterYearsBetweenFrame: org.apache.spark.sql.DataFrame \u003d [encounter_id: string, person_id: string ... 8 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1503436848322_-1538439700",
      "id": "20170822-212048_943543193",
      "dateCreated": "Aug 22, 2017 9:20:48 PM",
      "dateStarted": "Aug 22, 2017 11:27:31 PM",
      "dateFinished": "Aug 22, 2017 11:27:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "        val encounterLifecycleFlagsFrame \u003d buildEncounterLifecycleFlagsFromYearsBetween(spark)\n",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:25:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nencounterLifecycleFlagsFrame: org.apache.spark.sql.DataFrame \u003d [encounter_id: string, person_id: string ... 14 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1503436858086_-1432987853",
      "id": "20170822-212058_487030716",
      "dateCreated": "Aug 22, 2017 9:20:58 PM",
      "dateStarted": "Aug 22, 2017 11:27:33 PM",
      "dateFinished": "Aug 22, 2017 11:27:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "        val encounterLifecycleStateFrame \u003d buildEncounterLifecycleStateFromFlags(spark)",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:25:02 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\nencounterLifecycleStateFrame: org.apache.spark.sql.DataFrame \u003d [encounter_id: string, person_id: string ... 22 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1503436865346_600025335",
      "id": "20170822-212105_1374486951",
      "dateCreated": "Aug 22, 2017 9:21:05 PM",
      "dateStarted": "Aug 22, 2017 11:27:40 PM",
      "dateFinished": "Aug 22, 2017 11:27:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "WRITE",
      "text": "writePatientLifecycleDataToEncounter(spark,encounterLifecycleStateFrame,esIndexName,esServer)",
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:27:55 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job 4 cancelled part of cancelled job group zeppelin-20170822-212118_1031104713\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:788)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:788)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1625)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n  at org.elasticsearch.spark.rdd.EsSpark$.doSaveToEs(EsSpark.scala:102)\n  at org.elasticsearch.spark.rdd.EsSpark$.saveToEsWithMeta(EsSpark.scala:85)\n  at org.elasticsearch.spark.rdd.EsSpark$.saveToEsWithMeta(EsSpark.scala:82)\n  at org.elasticsearch.spark.package$SparkPairRDDFunctions.saveToEsWithMeta(package.scala:73)\n  at writePatientLifecycleDataToEncounter(\u003cconsole\u003e:134)\n  ... 58 elided\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1503436878352_1313966052",
      "id": "20170822-212118_1031104713",
      "dateCreated": "Aug 22, 2017 9:21:18 PM",
      "dateStarted": "Aug 22, 2017 11:27:56 PM",
      "dateFinished": "Aug 22, 2017 11:35:02 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "dateUpdated": "Aug 22, 2017 11:25:02 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1503438217457_-985575264",
      "id": "20170822-214337_1012226917",
      "dateCreated": "Aug 22, 2017 9:43:37 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "/patient_lifecycle/v1.0.1/Encounter Patient Lifecycle Processing",
  "id": "2CQGAAW46",
  "angularObjects": {
    "2CU4Q3DJG:shared_process": [],
    "2CSQ8A121:shared_process": [],
    "2CTXDQFQV:shared_process": [],
    "2CS3V8QAH:shared_process": [],
    "2CTM3MGXS:shared_process": [],
    "2CS62SXBQ:shared_process": [],
    "2CQ9A889G:shared_process": [],
    "2CU51G62A:shared_process": [],
    "2CRE21WXT:shared_process": []
  },
  "config": {
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {}
}